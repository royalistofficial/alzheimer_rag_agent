{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fbc5b8",
   "metadata": {},
   "source": [
    "# RAG-агент: поиск терапевтических мишеней болезни Альцгеймера\n",
    "\n",
    "**Тестовое задание — Стажёры 2026**\n",
    "\n",
    "---\n",
    "\n",
    "## Описание проекта\n",
    "\n",
    "Болезнь Альцгеймера (AD) — наиболее распространённая форма деменции, затрагивающая более 55 миллионов человек по всему миру. Несмотря на десятилетия исследований, одобренных заболевание-модифицирующих терапий крайне мало, а фундаментальное понимание патогенеза продолжает развиваться. Одним из ключевых этапов разработки лекарств является **идентификация и валидация терапевтических мишеней** — белков, рецепторов, ферментов и сигнальных путей, воздействие на которые может замедлить или остановить нейродегенерацию.\n",
    "\n",
    "Данный проект реализует **RAG-агента** (Retrieval-Augmented Generation), который:\n",
    "\n",
    "1. **Автоматически собирает** научные статьи из PubMed по релевантным запросам\n",
    "2. **Строит базу знаний** с семантическим и ключевым поиском (hybrid search)\n",
    "3. **Отвечает на вопросы** исследователей с указанием конкретных источников `[PMID:XXXXX]`\n",
    "4. **Оценивает качество** ответов по нескольким метрикам, включая retrieval-метрики на gold standard\n",
    "\n",
    "### Архитектура\n",
    "\n",
    "```\n",
    "PubMed API → XML Parsing → Text Cleaning → Chunking\n",
    "                                              |\n",
    "                                    Sentence-Transformers → ChromaDB (dense)\n",
    "                                              +\n",
    "                                         BM25 (sparse)\n",
    "                                              |\n",
    "                            Hybrid Search (RRF) → Cross-Encoder Re-ranking\n",
    "                                              |\n",
    "                                    Local LLM (HuggingFace) → Answer + Citations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Содержание\n",
    "\n",
    "| # | Часть | Описание |\n",
    "|---|-------|----------|\n",
    "| 1 | [Подготовка данных](#part1) | Сбор статей с PubMed и PMC, очистка, retry-логика |\n",
    "| 2 | [EDA](#part2) | Анализ корпуса: распределения, TF-IDF, co-occurrence мишеней |\n",
    "| 3 | [Векторная БД](#part3) | Parent-child чанкинг, эмбеддинги, ChromaDB + BM25 |\n",
    "| 4 | [RAG Pipeline](#part4) | Hybrid retrieval → Cross-encoder → LLM → Метрики + Gold standard |\n",
    "| 5 | [Интерфейс](#part5) | Интерактивные виджеты для исследователей |\n",
    "| 6 | [Теор. вопросы](#part6) | Модальности, архитектура, обоснование выбора |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    requests \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    wordcloud \\\n",
    "    scikit-learn \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    chromadb \\\n",
    "    sentence-transformers \\\n",
    "    langchain-text-splitters \\\n",
    "    transformers \\\n",
    "    torch \\\n",
    "    accelerate \\\n",
    "    ipywidgets \\\n",
    "    rank-bm25 \\\n",
    "    tiktoken \\\n",
    "    ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccf1af",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, os, gc, hashlib, unicodedata, warnings, shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import requests as req\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "import tiktoken\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75b22e",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "# Часть 1. Сбор и подготовка данных\n",
    "\n",
    "## Методология сбора\n",
    "\n",
    "Для построения базы знаний используется **NCBI E-utilities API** — программный интерфейс к PubMed (>36 млн записей). Реализация без внешних библиотек для PubMed — чистый `requests` + `xml.etree.ElementTree`.\n",
    "\n",
    "### Стратегия поиска\n",
    "\n",
    "Используем **15 поисковых запросов** с разным фокусом. `MAX_PER_QUERY = 50` даёт до 750(636) кандидатов до дедупликации — достаточный корпус для надёжного RAG.\n",
    "\n",
    "### Pipeline обработки\n",
    "\n",
    "```\n",
    "ESearch (JSON) → список PMID\n",
    "       |\n",
    "EFetch (XML) → метаданные + абстракты\n",
    "       |\n",
    "PMC Full-text (XML) → Introduction + Conclusion\n",
    "       |\n",
    "Очистка: inline-ссылки [1,2], (Smith et al.), URL, спецсимволы\n",
    "       |\n",
    "Фильтрация: минимум 100 символов + >= 2 ключевых слова\n",
    "       |\n",
    "data/clean_articles.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1fd3fc",
   "metadata": {},
   "source": [
    "### 1.1 Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = 'email@example.com'\n",
    "TOOL_NAME = 'alzheimer_rag_agent'\n",
    "\n",
    "BASE_URL = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils'\n",
    "ESEARCH_URL = f'{BASE_URL}/esearch.fcgi'\n",
    "EFETCH_URL  = f'{BASE_URL}/efetch.fcgi'\n",
    "\n",
    "QUERIES = [\n",
    "    \"Alzheimer's disease targets\",\n",
    "    \"Alzheimer therapeutic targets\",\n",
    "    \"Alzheimer drug targets\",\n",
    "    \"Alzheimer's disease drug discovery targets\",\n",
    "    \"novel targets Alzheimer treatment\",\n",
    "    \"amyloid beta therapeutic target Alzheimer\",\n",
    "    \"tau phosphorylation drug target Alzheimer\",\n",
    "    \"neuroinflammation targets Alzheimer's disease\",\n",
    "    \"TREM2 Alzheimer therapeutic\",\n",
    "    \"BACE1 inhibitor Alzheimer clinical\",\n",
    "    \"autophagy target Alzheimer neurodegeneration\",\n",
    "    \"synaptic dysfunction Alzheimer drug target\",\n",
    "    \"gut-brain axis Alzheimer therapeutic\",\n",
    "    \"epigenetic targets Alzheimer's disease\",\n",
    "    \"mitochondrial dysfunction Alzheimer drug\",\n",
    "]\n",
    "\n",
    "MAX_PER_QUERY = 50\n",
    "OUTPUT_DIR = Path('data')\n",
    "RAW_JSON   = OUTPUT_DIR / 'raw_articles.json'\n",
    "CLEAN_JSON = OUTPUT_DIR / 'clean_articles.json'\n",
    "STATS_JSON = OUTPUT_DIR / 'collection_stats.json'\n",
    "\n",
    "print(f'Запросов: {len(QUERIES)}')\n",
    "print(f'Max на запрос: {MAX_PER_QUERY}')\n",
    "print(f'Потенциальный охват: до {len(QUERIES) * MAX_PER_QUERY} статей (до дедупликации)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743f126",
   "metadata": {},
   "source": [
    "### 1.2 Датакласс `Article`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    pmid: str\n",
    "    title: str = ''\n",
    "    authors: list[str] = field(default_factory=list)\n",
    "    journal: str = ''\n",
    "    year: str = ''\n",
    "    doi: str = ''\n",
    "    abstract: str = ''\n",
    "    introduction: str = ''\n",
    "    conclusion: str = ''\n",
    "    pmc_id: str = ''\n",
    "    query_source: str = ''\n",
    "    full_text_available: bool = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4d191",
   "metadata": {},
   "source": [
    "### 1.3 HTTP-хелпер с retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0b2c9",
   "metadata": {},
   "source": [
    "### `api_get`\n",
    "\n",
    "HTTP GET-запрос к NCBI API с автоматическим retry при ошибках и rate-limit.\n",
    "\n",
    "**Вход:** `url` (str), `params` (dict), `timeout` (int), `max_retries` (int)  \n",
    "**Выход:** `requests.Response` — ответ сервера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_get(url: str, params: dict, timeout: int = 30, max_retries: int = 3):\n",
    "    params = {**params, 'email': EMAIL, 'tool': TOOL_NAME}\n",
    "    last_exc = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = req.get(url, params=params, timeout=timeout)\n",
    "            if resp.status_code == 429:\n",
    "                wait = 2 ** attempt\n",
    "                print(f'  [rate limit] ожидание {wait}s (попытка {attempt+1})')\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "        except req.exceptions.RequestException as exc:\n",
    "            last_exc = exc\n",
    "            wait = 2 ** attempt\n",
    "            print(f'  [retry {attempt+1}/{max_retries}] {exc} -> ожидание {wait}s')\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f'API недоступен после {max_retries} попыток: {last_exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f571e9",
   "metadata": {},
   "source": [
    "### 1.4 Поиск PMID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d2894",
   "metadata": {},
   "source": [
    "### `search_pubmed`\n",
    "\n",
    "Ищет PMID статей в PubMed по текстовому запросу через ESearch API.\n",
    "\n",
    "**Вход:** `query` (str) — поисковый запрос, `max_results` (int)  \n",
    "**Выход:** `list[str]` — список найденных PMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98401833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pubmed(query: str, max_results: int = 100) -> list[str]:\n",
    "    resp = api_get(ESEARCH_URL, {\n",
    "        'db': 'pubmed', 'term': query, 'retmax': max_results,\n",
    "        'sort': 'relevance', 'datetype': 'pdat',\n",
    "        'mindate': '2010', 'maxdate': '2026', 'retmode': 'json',\n",
    "    })\n",
    "    pmids = resp.json().get('esearchresult', {}).get('idlist', [])\n",
    "    print(f'  \"{query}\" -> {len(pmids)} PMID')\n",
    "    return pmids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee318c",
   "metadata": {},
   "source": [
    "### 1.5 Сбор уникальных PMID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b14c1",
   "metadata": {},
   "source": [
    "### `collect_all_pmids`\n",
    "\n",
    "Собирает уникальные PMID по всем поисковым запросам из QUERIES, дедуплицируя результаты.\n",
    "\n",
    "**Вход:** нет (использует глобальный QUERIES)  \n",
    "**Выход:** `dict[str, str]` — словарь {pmid: query_source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa766c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_pmids() -> dict[str, str]:\n",
    "    pmid_to_query = {}\n",
    "    for q in QUERIES:\n",
    "        for pmid in search_pubmed(q, MAX_PER_QUERY):\n",
    "            if pmid not in pmid_to_query:\n",
    "                pmid_to_query[pmid] = q\n",
    "        time.sleep(0.4)\n",
    "    print(f'\\nУникальных PMID: {len(pmid_to_query)}')\n",
    "    return pmid_to_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6311efc",
   "metadata": {},
   "source": [
    "### 1.6 Загрузка PubMed XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c5673",
   "metadata": {},
   "source": [
    "### `fetch_pubmed_xml`\n",
    "\n",
    "Загружает XML-метаданные статей пакетами через EFetch API.\n",
    "\n",
    "**Вход:** `pmids` (list[str]) — список PMID, `batch_size` (int)  \n",
    "**Выход:** `list[Element]` — список XML-элементов PubmedArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_xml(pmids: list[str], batch_size: int = 100) -> list:\n",
    "    all_articles = []\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[i:i + batch_size]\n",
    "        print(f'  Загрузка {i+1}-{i+len(batch)} из {len(pmids)}')\n",
    "        resp = api_get(EFETCH_URL, {\n",
    "            'db': 'pubmed', 'id': ','.join(batch),\n",
    "            'rettype': 'xml', 'retmode': 'xml',\n",
    "        })\n",
    "        root = ET.fromstring(resp.content)\n",
    "        all_articles.extend(root.findall('.//PubmedArticle'))\n",
    "        time.sleep(0.4)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a351a9",
   "metadata": {},
   "source": [
    "### 1.7 Парсинг PubMed XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5c215",
   "metadata": {},
   "source": [
    "### `_get_text`, `_get_all_text`, `parse_pubmed_article`, `parse_all_articles`\n",
    "\n",
    "- `_get_text` — извлекает текст из XML-элемента по XPath. **Вход:** Element, path. **Выход:** str\n",
    "- `_get_all_text` — собирает весь текст из элемента рекурсивно. **Вход:** Element. **Выход:** str\n",
    "- `parse_pubmed_article` — парсит один PubmedArticle XML-элемент в словарь метаданных. **Вход:** XML Element. **Выход:** dict | None\n",
    "- `parse_all_articles` — парсит список XML-элементов в список Article. **Вход:** list[Element], dict pmid→query. **Выход:** list[Article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_text(el, path, default=''):\n",
    "    e = el.find(path)\n",
    "    return e.text.strip() if e is not None and e.text else default\n",
    "\n",
    "def _get_all_text(el):\n",
    "    if el is None: return ''\n",
    "    return ' '.join(c.strip() for c in el.itertext() if c.strip())\n",
    "\n",
    "def parse_pubmed_article(elem) -> dict | None:\n",
    "    citation = elem.find('MedlineCitation')\n",
    "    article = citation.find('Article') if citation is not None else None\n",
    "    if citation is None or article is None: return None\n",
    "    pmid = _get_text(citation, 'PMID')\n",
    "    title = _get_all_text(article.find('ArticleTitle'))\n",
    "    authors = []\n",
    "    al = article.find('AuthorList')\n",
    "    if al is not None:\n",
    "        for a in al.findall('Author'):\n",
    "            last = _get_text(a, 'LastName')\n",
    "            if last: authors.append(f\"{last} {_get_text(a, 'Initials')}\")\n",
    "    journal = ''\n",
    "    je = article.find('Journal')\n",
    "    if je is not None:\n",
    "        journal = _get_text(je, 'Title') or _get_text(je, 'ISOAbbreviation')\n",
    "    year = ''\n",
    "    pd_ = article.find('.//Journal/JournalIssue/PubDate')\n",
    "    if pd_ is not None:\n",
    "        year = _get_text(pd_, 'Year')\n",
    "        if not year:\n",
    "            md_ = _get_text(pd_, 'MedlineDate')\n",
    "            m = re.match(r'(\\d{4})', md_) if md_ else None\n",
    "            if m: year = m.group(1)\n",
    "    if not year:\n",
    "        ad = article.find('ArticleDate')\n",
    "        if ad is not None: year = _get_text(ad, 'Year')\n",
    "    abstract_parts = []\n",
    "    ae = article.find('Abstract')\n",
    "    if ae is not None:\n",
    "        for at in ae.findall('AbstractText'):\n",
    "            label = at.get('Label', '')\n",
    "            text = _get_all_text(at)\n",
    "            if text: abstract_parts.append(f'{label}: {text}' if label else text)\n",
    "    abstract = ' '.join(abstract_parts)\n",
    "    doi = ''\n",
    "    for eloc in article.findall('ELocationID'):\n",
    "        if eloc.get('EIdType') == 'doi' and eloc.text: doi = eloc.text.strip(); break\n",
    "    if not doi:\n",
    "        pdata = elem.find('PubmedData')\n",
    "        if pdata:\n",
    "            for aid in pdata.findall('.//ArticleId'):\n",
    "                if aid.get('IdType') == 'doi' and aid.text: doi = aid.text.strip(); break\n",
    "    pmc_id = ''\n",
    "    pdata = elem.find('PubmedData')\n",
    "    if pdata:\n",
    "        for aid in pdata.findall('.//ArticleId'):\n",
    "            if aid.get('IdType') == 'pmc' and aid.text: pmc_id = aid.text.strip(); break\n",
    "    return {'pmid': pmid, 'title': title, 'authors': authors, 'journal': journal,\n",
    "            'year': year, 'doi': doi, 'abstract': abstract, 'pmc_id': pmc_id}\n",
    "\n",
    "def parse_all_articles(xml_articles, pmid_to_query) -> list[Article]:\n",
    "    articles = []\n",
    "    for elem in xml_articles:\n",
    "        p = parse_pubmed_article(elem)\n",
    "        if p is None: continue\n",
    "        articles.append(Article(**p, query_source=pmid_to_query.get(p['pmid'], '')))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdb056",
   "metadata": {},
   "source": [
    "### 1.8 PMC full-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe5e60",
   "metadata": {},
   "source": [
    "### `fetch_pmc_sections`, `_parse_pmc_xml`, `_extract_section_text`, `enrich_with_fulltext`\n",
    "\n",
    "- `fetch_pmc_sections` — загружает Introduction и Conclusion из PMC full-text. **Вход:** pmc_id (str). **Выход:** tuple[str, str]\n",
    "- `_parse_pmc_xml` — парсит PMC XML и извлекает секции intro/conclusion. **Вход:** xml_text (str). **Выход:** tuple[str, str]\n",
    "- `_extract_section_text` — извлекает текст из XML-секции, пропуская формулы и таблицы. **Вход:** Element. **Выход:** str\n",
    "- `enrich_with_fulltext` — обогащает список Article полнотекстовыми секциями из PMC. **Вход:** list[Article]. **Выход:** None (модифицирует in-place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pmc_sections(pmc_id: str) -> tuple[str, str]:\n",
    "    if not pmc_id: return '', ''\n",
    "    try:\n",
    "        resp = api_get(EFETCH_URL, {'db': 'pmc', 'id': pmc_id.replace('PMC',''), 'rettype': 'xml'})\n",
    "    except Exception as e:\n",
    "        print(f'  [warn] {pmc_id}: {e}')\n",
    "        return '', ''\n",
    "    return _parse_pmc_xml(resp.text)\n",
    "\n",
    "def _parse_pmc_xml(xml_text):\n",
    "    try: root = ET.fromstring(xml_text)\n",
    "    except ET.ParseError: return '', ''\n",
    "    intro, concl = '', ''\n",
    "    for sec in root.iter('sec'):\n",
    "        st = (sec.get('sec-type') or '').lower()\n",
    "        te = sec.find('title')\n",
    "        t = (te.text or '').lower().strip() if te is not None else ''\n",
    "        is_i = st in ('intro','introduction') or 'introduction' in t or 'background' in t\n",
    "        is_c = st in ('conclusion','conclusions') or 'conclusion' in t\n",
    "        if is_i and not intro: intro = _extract_section_text(sec)\n",
    "        elif is_c and not concl: concl = _extract_section_text(sec)\n",
    "    return intro, concl\n",
    "\n",
    "def _extract_section_text(el):\n",
    "    skip = {'xref','table-wrap','fig','disp-formula','inline-formula','ext-link'}\n",
    "    parts = []\n",
    "    for n in el.iter():\n",
    "        if n.tag in skip: continue\n",
    "        if n.text: parts.append(n.text.strip())\n",
    "        if n.tail: parts.append(n.tail.strip())\n",
    "    return ' '.join(filter(None, parts))\n",
    "\n",
    "def enrich_with_fulltext(articles):\n",
    "    pmc = [a for a in articles if a.pmc_id]\n",
    "    print(f'PMC full-text: {len(pmc)} из {len(articles)} статей')\n",
    "    for i, a in enumerate(pmc, 1):\n",
    "        print(f'  [{i}/{len(pmc)}] {a.pmc_id}', end=' ')\n",
    "        intro, concl = fetch_pmc_sections(a.pmc_id)\n",
    "        a.introduction, a.conclusion = intro, concl\n",
    "        a.full_text_available = bool(intro or concl)\n",
    "        print('OK' if a.full_text_available else 'нет секций')\n",
    "        time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f227cb0",
   "metadata": {},
   "source": [
    "### 1.9 Очистка и фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571b7c9",
   "metadata": {},
   "source": [
    "### `clean_text`, `clean_articles`, `save_articles`\n",
    "\n",
    "- `clean_text` — очищает текст от inline-ссылок, URL, email и спецсимволов. **Вход:** str. **Выход:** str\n",
    "- `clean_articles` — фильтрует статьи по длине абстракта (≥100 симв.). **Вход:** list[Article]. **Выход:** list[Article]\n",
    "- `save_articles` — сохраняет список Article в JSON-файл. **Вход:** list[Article], Path. **Выход:** None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cff793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text: return ''\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'\\[\\s*\\d+(?:\\s*[,\\-\\u2013]\\s*\\d+)*\\s*\\]', '', text)\n",
    "    text = re.sub(r\"\\(\\s*[A-Z][a-zA-Z\\-']+(?:\\s+(?:et\\s+al\\.?|and\\s+[A-Z][a-zA-Z\\-']+))?(?:,?\\s*\\d{4}[a-z]?)\\s*\\)\", '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'doi:\\s*\\S+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
    "    text = re.sub(r\"[^\\w\\s.,;:!?()\\-\\u2013/'+\\u03b1-\\u03c9]\", ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def clean_articles(articles):\n",
    "    cleaned, rm_short = [], 0\n",
    "    for a in articles:\n",
    "        a.title = clean_text(a.title)\n",
    "        a.abstract = clean_text(a.abstract)\n",
    "        a.introduction = clean_text(a.introduction)\n",
    "        a.conclusion = clean_text(a.conclusion)\n",
    "        if len(a.abstract) < 100: rm_short += 1; continue\n",
    "        cleaned.append(a)\n",
    "    print(f'Оставлено: {len(cleaned)} | удалено коротких: {rm_short}')\n",
    "    return cleaned\n",
    "\n",
    "def save_articles(articles, path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump([asdict(a) for a in articles], f, ensure_ascii=False, indent=2)\n",
    "    print(f'Сохранено: {path} ({len(articles)} статей)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa6443",
   "metadata": {},
   "source": [
    "### 1.10 Запуск сбора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b08e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('  ФАЗА 1: Сбор данных с PubMed')\n",
    "print('=' * 60)\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "    print(f'Старые данные удалены: {OUTPUT_DIR}/')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pmid_to_query = collect_all_pmids()\n",
    "\n",
    "print('\\nЗагрузка метаданных...')\n",
    "xml_articles = fetch_pubmed_xml(list(pmid_to_query.keys()))\n",
    "articles = parse_all_articles(xml_articles, pmid_to_query)\n",
    "print(f'Загружено: {len(articles)} статей')\n",
    "\n",
    "print('\\nPMC full-text...')\n",
    "enrich_with_fulltext(articles)\n",
    "save_articles(articles, RAW_JSON)\n",
    "raw_count = len(articles)\n",
    "\n",
    "print('\\nОчистка...')\n",
    "articles = clean_articles(articles)\n",
    "save_articles(articles, CLEAN_JSON)\n",
    "\n",
    "ft = sum(1 for a in articles if a.full_text_available)\n",
    "print(f'\\nИтого: {raw_count} -> {len(articles)} статей (с full-text: {ft})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c71c7d",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "# Часть 2. Эксплораторный анализ (EDA)\n",
    "\n",
    "## Цели анализа\n",
    "\n",
    "- **Временное покрытие**: из каких лет статьи? Есть ли тренд роста интереса?\n",
    "- **Объём текстов**: достаточно ли контекста для RAG? Какая доля full-text?\n",
    "- **Ключевые термины**: какие мишени и механизмы чаще всего упоминаются?\n",
    "- **Co-occurrence**: какие мишени встречаются совместно — матрица совместной встречаемости\n",
    "- **Источники**: в каких журналах больше всего релевантных статей?\n",
    "\n",
    "Результаты EDA помогают настроить параметры чанкинга и оценить покрытие базы знаний.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de112f36",
   "metadata": {},
   "source": [
    "### 2.1 Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEAN_JSON, 'r', encoding='utf-8') as f:\n",
    "    df = pd.DataFrame(json.load(f))\n",
    "\n",
    "def combine_text(row):\n",
    "    return ' '.join(p for p in [row.get('title',''), row.get('abstract',''),\n",
    "                                 row.get('introduction',''), row.get('conclusion','')] if p)\n",
    "\n",
    "df['combined_text'] = df.apply(combine_text, axis=1)\n",
    "df['abstract_words'] = df['abstract'].apply(lambda x: len(x.split()) if x else 0)\n",
    "df['combined_words'] = df['combined_text'].apply(lambda x: len(x.split()))\n",
    "df['has_ft'] = df['full_text_available'].map({True: 'С full-text', False: 'Только абстракт'})\n",
    "\n",
    "print(f'Статей: {len(df)}')\n",
    "print(f'  С full-text: {df[\"full_text_available\"].sum()}')\n",
    "print(f'  Годы: {df[\"year\"].min()} - {df[\"year\"].max()}')\n",
    "print(f'  Журналов: {df[\"journal\"].nunique()}')\n",
    "df[['pmid','title','year','journal','full_text_available']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d3d3d",
   "metadata": {},
   "source": [
    "### 2.2 Распределение по годам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = df['year'].value_counts().sort_index()\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(year_counts.index, year_counts.values,\n",
    "              color=sns.color_palette('viridis', len(year_counts)), edgecolor='white')\n",
    "for b, v in zip(bars, year_counts.values):\n",
    "    ax.text(b.get_x()+b.get_width()/2, b.get_height()+0.3,\n",
    "            str(v), ha='center', fontweight='bold', fontsize=10)\n",
    "ax.set_xlabel('Год'); ax.set_ylabel('Кол-во статей')\n",
    "ax.set_title('Распределение статей по годам публикации')\n",
    "plt.xticks(rotation=45); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f044be4",
   "metadata": {},
   "source": [
    "### 2.3 Длины текстов и доля full-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "sns.histplot(df['abstract_words'], bins=30, kde=True, ax=axes[0], color='steelblue')\n",
    "axes[0].axvline(df['abstract_words'].median(), color='red', ls='--',\n",
    "                label=f'Медиана: {df[\"abstract_words\"].median():.0f}')\n",
    "axes[0].set_title('Длина абстрактов (слова)'); axes[0].legend()\n",
    "\n",
    "sns.boxplot(data=df, x='has_ft', y='combined_words', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Full-text vs абстракт'); axes[1].set_xlabel('')\n",
    "\n",
    "ft_vals = df['full_text_available'].value_counts()\n",
    "ft_labels = ['Full-text' if idx else 'Абстракт' for idx in ft_vals.index]\n",
    "ft_colors = ['#66b3ff' if idx else '#ff9999' for idx in ft_vals.index]\n",
    "axes[2].pie(ft_vals.values, labels=ft_labels,\n",
    "            autopct='%1.1f%%', colors=ft_colors, startangle=90)\n",
    "axes[2].set_title('Доля full-text')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d4fee",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF + Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf676e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=200, stop_words='english',\n",
    "    ngram_range=(1, 2), min_df=3, max_df=0.75,\n",
    "    token_pattern=r'[a-zA-Z\\u03b1-\\u03c9][a-zA-Z\\u03b1-\\u03c9\\-]{2,}',\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'].tolist())\n",
    "features = vectorizer.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_idx = mean_tfidf.argsort()[::-1][:40]\n",
    "top_terms = {features[i]: mean_tfidf[i] for i in top_idx}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "t30 = dict(list(top_terms.items())[:30])\n",
    "sns.barplot(y=list(t30.keys()), x=list(t30.values()), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Топ-30 терминов (TF-IDF)'); axes[0].set_xlabel('Средний TF-IDF')\n",
    "\n",
    "wc = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=80)\n",
    "wc.generate_from_frequencies(top_terms)\n",
    "axes[1].imshow(wc, interpolation='bilinear'); axes[1].axis('off')\n",
    "axes[1].set_title('Word Cloud (TF-IDF weighted)')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637da87f",
   "metadata": {},
   "source": [
    "### 2.5 Топ журналов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_j = df['journal'].value_counts().head(15)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x=top_j.values, y=top_j.index, palette='mako', ax=ax)\n",
    "ax.set_title('Топ-15 журналов'); ax.set_xlabel('Статей')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a9306",
   "metadata": {},
   "source": [
    "### 2.6 Co-occurrence матрица терапевтических мишеней\n",
    "\n",
    "Ключевые терапевтические мишени болезни Альцгеймера часто встречаются вместе в одной статье.\n",
    "Матрица совместной встречаемости (heatmap) и столбчатая диаграмма топ-15 пар показывают,\n",
    "какие мишени исследуются совместно — это помогает понять структуру данных перед\n",
    "построением RAG-системы и выявить тематические кластеры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx = mean_tfidf.argsort()[::-1][:15]\n",
    "TARGET_TERMS = [features[i] for i in top_idx]\n",
    "\n",
    "n = len(TARGET_TERMS)\n",
    "cooc = np.zeros((n, n), dtype=int)\n",
    "for _, row in df.iterrows():\n",
    "    txt = row['combined_text'].lower()\n",
    "    present = [i for i, t in enumerate(TARGET_TERMS) if t in txt]\n",
    "    for i in present:\n",
    "        for j in present:\n",
    "            if i != j:\n",
    "                cooc[i, j] += 1\n",
    "\n",
    "cooc_df = pd.DataFrame(cooc, index=TARGET_TERMS, columns=TARGET_TERMS)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "sns.heatmap(cooc_df, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title('Co-occurrence мишеней (кол-во статей)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        if cooc[i, j] > 0:\n",
    "            pairs.append((TARGET_TERMS[i], TARGET_TERMS[j], cooc[i, j]))\n",
    "pairs.sort(key=lambda x: -x[2])\n",
    "top_pairs = pairs[:15]\n",
    "\n",
    "pair_labels = [f'{a} + {b}' for a, b, _ in top_pairs]\n",
    "pair_vals = [v for _, _, v in top_pairs]\n",
    "sns.barplot(x=pair_vals, y=pair_labels, palette='rocket_r', ax=axes[1])\n",
    "axes[1].set_title('Топ-15 пар мишеней по частоте совместного упоминания')\n",
    "axes[1].set_xlabel('Кол-во статей')\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\nНаиболее часто исследуемые пары мишеней:')\n",
    "for a, b, v in top_pairs[:5]:\n",
    "    print(f'  {a} + {b}: {v} статей')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246e7ce",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "# Часть 3. Векторная база данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a8462",
   "metadata": {},
   "source": [
    "### 3.1 Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DIR = Path('vectordb')\n",
    "COLLECTION_NAME = 'alzheimer_children'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "\n",
    "CHILD_CHUNK_SIZE = 300\n",
    "CHILD_CHUNK_OVERLAP = 50\n",
    "MIN_CHUNK_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b3c21",
   "metadata": {},
   "source": [
    "### 3.2 Построение иерархии Parent → Children\n",
    "\n",
    "Каждая секция статьи становится **parent-документом**. Внутри parent нарезаются мелкие **child-чанки** для индексации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEAN_JSON, 'r', encoding='utf-8') as f:\n",
    "    articles_data = json.load(f)\n",
    "\n",
    "parents = {}\n",
    "\n",
    "for art in articles_data:\n",
    "    base_meta = {k: art.get(k, '') for k in ['pmid','title','journal','year','doi']}\n",
    "    base_meta['authors'] = ', '.join(art.get('authors', [])[:3])\n",
    "    for section in ['abstract', 'introduction', 'conclusion']:\n",
    "        text = art.get(section, '')\n",
    "        if not text or len(text) < MIN_CHUNK_LENGTH:\n",
    "            continue\n",
    "        parent_id = f\"{art['pmid']}_{section}\"\n",
    "        parents[parent_id] = {\n",
    "            'text': text,\n",
    "            'metadata': {**base_meta, 'section': section, 'parent_id': parent_id},\n",
    "        }\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHILD_CHUNK_SIZE,\n",
    "    chunk_overlap=CHILD_CHUNK_OVERLAP,\n",
    "    separators=['. ', '; ', ', ', ' ', ''],\n",
    "    keep_separator=True,\n",
    ")\n",
    "\n",
    "children = []\n",
    "\n",
    "for pid, parent in parents.items():\n",
    "    splits = child_splitter.split_text(parent['text'])\n",
    "    for i, chunk_text in enumerate(splits):\n",
    "        if len(chunk_text.strip()) < MIN_CHUNK_LENGTH:\n",
    "            continue\n",
    "        child_id = hashlib.md5(f\"{pid}_{i}\".encode()).hexdigest()[:12]\n",
    "        children.append({\n",
    "            'id': child_id,\n",
    "            'text': chunk_text.strip(),\n",
    "            'parent_id': pid,\n",
    "            'metadata': {\n",
    "                **parent['metadata'],\n",
    "                'parent_id': pid,\n",
    "                'child_index': i,\n",
    "                'child_chars': len(chunk_text),\n",
    "            },\n",
    "        })\n",
    "\n",
    "unique_pmids = len(set(p['metadata']['pmid'] for p in parents.values()))\n",
    "sec_counts = Counter(p['metadata']['section'] for p in parents.values())\n",
    "child_lens = [len(c['text']) for c in children]\n",
    "parent_lens = [len(p['text']) for p in parents.values()]\n",
    "\n",
    "print(f'Иерархия построена:')\n",
    "print(f'  Статей: {unique_pmids}')\n",
    "print(f'  Parents (секций): {len(parents)}')\n",
    "for s, cnt in sorted(sec_counts.items()): print(f'    {s}: {cnt}')\n",
    "print(f'  Children (чанков): {len(children)}')\n",
    "print(f'  Parent длина: мин={min(parent_lens)}, медиана={sorted(parent_lens)[len(parent_lens)//2]}, макс={max(parent_lens)}')\n",
    "print(f'  Child длина:  мин={min(child_lens)}, медиана={sorted(child_lens)[len(child_lens)//2]}, макс={max(child_lens)}')\n",
    "print(f'  Среднее children/parent: {len(children)/len(parents):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a46ac",
   "metadata": {},
   "source": [
    "### 3.3 Эмбеддинги children\n",
    "\n",
    "Индексируются **только children** — маленькие чанки для точного поиска. Parents хранятся отдельно в памяти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f'Модель: {EMBEDDING_MODEL} ({encoder.get_sentence_embedding_dimension()}d)')\n",
    "\n",
    "child_texts = [c['text'] for c in children]\n",
    "child_embeddings = encoder.encode(child_texts, batch_size=64,\n",
    "                                   show_progress_bar=True,\n",
    "                                   normalize_embeddings=True).tolist()\n",
    "print(f'Эмбеддинги children: {len(child_embeddings)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765761c",
   "metadata": {},
   "source": [
    "### 3.4 ChromaDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36020e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHROMA_DIR.exists():\n",
    "    shutil.rmtree(CHROMA_DIR)\n",
    "    print(f'Старая БД удалена: {CHROMA_DIR}/')\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMA_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False),\n",
    ")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        'hnsw:space': 'cosine',\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'architecture': 'hierarchical_parent_child',\n",
    "        'child_chunk_size': CHILD_CHUNK_SIZE,\n",
    "    },\n",
    ")\n",
    "\n",
    "for i in range(0, len(children), 500):\n",
    "    end = min(i + 500, len(children))\n",
    "    batch = children[i:end]\n",
    "    metas = [{k: (v if isinstance(v, (str, int, float, bool)) else str(v))\n",
    "              for k, v in c['metadata'].items()} for c in batch]\n",
    "    collection.add(\n",
    "        ids=[c['id'] for c in batch],\n",
    "        documents=[c['text'] for c in batch],\n",
    "        embeddings=child_embeddings[i:end],\n",
    "        metadatas=metas,\n",
    "    )\n",
    "\n",
    "print(f'ChromaDB: {collection.count()} children в \"{COLLECTION_NAME}\"')\n",
    "print(f'Parents в памяти: {len(parents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e71cb7",
   "metadata": {},
   "source": [
    "### 3.5 BM25 индекс (children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012c598",
   "metadata": {},
   "source": [
    "### `bm25_tokenize`\n",
    "\n",
    "Токенизирует текст для BM25: извлекает слова длиной ≥3 символов в нижнем регистре.\n",
    "\n",
    "**Вход:** `text` (str)  \n",
    "**Выход:** `list[str]` — список токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b407af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenize(text):\n",
    "    return re.findall(r'[a-zA-Z\\u03b1-\\u03c9][a-zA-Z\\u03b1-\\u03c9\\-]{2,}', text.lower())\n",
    "\n",
    "bm25_corpus = [bm25_tokenize(c['text']) for c in children]\n",
    "bm25_index = BM25Okapi(bm25_corpus)\n",
    "print(f'BM25: {len(bm25_corpus)} children')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3381c",
   "metadata": {},
   "source": [
    "### 3.6 Функции поиска: child → parent\n",
    "\n",
    "После нахождения релевантных **children** возвращаем их **parent** (полную секцию).\n",
    "Дедупликация по `parent_id` гарантирует, что одна секция не повторяется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10a67f",
   "metadata": {},
   "source": [
    "### `search_dense`, `search_bm25`, `search_hybrid_children`, `children_to_parents`\n",
    "\n",
    "- `search_dense` — dense-поиск по ChromaDB (cosine similarity). **Вход:** query (str), top_k (int). **Выход:** list[dict] с id, sim, meta, text\n",
    "- `search_bm25` — sparse-поиск BM25 по children. **Вход:** query (str), top_k (int). **Выход:** list[dict] с id, score, meta, text\n",
    "- `search_hybrid_children` — гибридный поиск (dense + BM25) с RRF fusion. **Вход:** query (str), top_k, dense_k, bm25_k, rrf_k. **Выход:** list[dict] ранжированных children\n",
    "- `children_to_parents` — агрегирует children-результаты в уникальные parent-секции. **Вход:** list[dict] child_results, max_parents. **Выход:** list[dict] parent-чанков с метаданными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dense(query, top_k=10):\n",
    "    q_emb = encoder.encode([query], normalize_embeddings=True).tolist()\n",
    "    res = collection.query(query_embeddings=q_emb, n_results=top_k,\n",
    "                           include=['documents','metadatas','distances'])\n",
    "    return [{'id': res['ids'][0][i],\n",
    "             'sim': 1 - res['distances'][0][i],\n",
    "             'meta': res['metadatas'][0][i],\n",
    "             'text': res['documents'][0][i]}\n",
    "            for i in range(len(res['ids'][0]))]\n",
    "\n",
    "\n",
    "def search_bm25(query, top_k=10):\n",
    "    scores = bm25_index.get_scores(bm25_tokenize(query))\n",
    "    top_idx = scores.argsort()[::-1][:top_k]\n",
    "    return [{'id': children[i]['id'], 'score': scores[i],\n",
    "             'meta': children[i]['metadata'], 'text': children[i]['text']}\n",
    "            for i in top_idx if scores[i] > 0]\n",
    "\n",
    "\n",
    "def search_hybrid_children(query, top_k=15, dense_k=20, bm25_k=20, rrf_k=60):\n",
    "    dense_res = search_dense(query, dense_k)\n",
    "    bm25_res = search_bm25(query, bm25_k)\n",
    "    rrf, data = {}, {}\n",
    "    for rank, r in enumerate(dense_res):\n",
    "        rrf[r['id']] = rrf.get(r['id'], 0) + 1 / (rrf_k + rank + 1)\n",
    "        data[r['id']] = r\n",
    "    for rank, r in enumerate(bm25_res):\n",
    "        rrf[r['id']] = rrf.get(r['id'], 0) + 1 / (rrf_k + rank + 1)\n",
    "        if r['id'] not in data: data[r['id']] = r\n",
    "    ranked = sorted(rrf.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{'child_id': cid, 'rrf_score': sc, **data[cid]} for cid, sc in ranked]\n",
    "\n",
    "\n",
    "def children_to_parents(child_results, max_parents=7):\n",
    "    parent_scores = {}\n",
    "    parent_children = {}\n",
    "    for cr in child_results:\n",
    "        pid = cr['meta'].get('parent_id', '')\n",
    "        if not pid or pid not in parents:\n",
    "            continue\n",
    "        score = cr.get('sim', cr.get('rrf_score', 0.5))\n",
    "        if pid not in parent_scores or score > parent_scores[pid]:\n",
    "            parent_scores[pid] = score\n",
    "        parent_children.setdefault(pid, []).append(cr['text'][:50])\n",
    "\n",
    "    ranked_pids = sorted(parent_scores.items(), key=lambda x: x[1], reverse=True)[:max_parents]\n",
    "    results = []\n",
    "    for pid, best_score in ranked_pids:\n",
    "        p = parents[pid]\n",
    "        results.append({\n",
    "            **p['metadata'],\n",
    "            'text': p['text'],\n",
    "            'similarity': round(best_score, 4),\n",
    "            'n_matching_children': len(parent_children.get(pid, [])),\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56900d6",
   "metadata": {},
   "source": [
    "### 3.7 Тестовый поиск: children → parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = 'BACE1 inhibitors clinical trials Alzheimer'\n",
    "print(f'Query: \"{test_q}\"\\n')\n",
    "\n",
    "child_res = search_hybrid_children(test_q, top_k=15)\n",
    "print(f'Children найдено: {len(child_res)}')\n",
    "for cr in child_res[:5]:\n",
    "    print(f'  rrf={cr[\"rrf_score\"]:.4f} | parent={cr[\"meta\"].get(\"parent_id\",\"\")} | {cr[\"text\"][:60]}...')\n",
    "\n",
    "parent_res = children_to_parents(child_res, max_parents=5)\n",
    "print(f'\\nParents (уникальные секции): {len(parent_res)}')\n",
    "for j, p in enumerate(parent_res, 1):\n",
    "    print(f'  [{j}] PMID:{p[\"pmid\"]} | {p[\"section\"]} | sim={p[\"similarity\"]:.3f} | '\n",
    "          f'children={p[\"n_matching_children\"]} | {len(p[\"text\"])} символов')\n",
    "    print(f'      {p[\"text\"][:100]}...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dde9e",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "# Часть 4. RAG Pipeline\n",
    "\n",
    "## Архитектура retrieval-augmented generation\n",
    "\n",
    "```\n",
    "Вопрос пользователя\n",
    "       |\n",
    "  Dense search + BM25  (Hybrid retrieval)\n",
    "         |\n",
    "  RRF Fusion (top 5×K кандидатов)\n",
    "         |\n",
    "  Cross-Encoder Re-ranking (top K)\n",
    "         |\n",
    "  Prompt Building (токены <CONTEXT>, <SOURCE>, <QUESTION>, <INSTRUCTION>)\n",
    "         |\n",
    "  Local LLM Generation\n",
    "         |\n",
    "  Postprocessing + Citation Extraction\n",
    "         |\n",
    "  Метрики: Faithfulness, Coverage, Accuracy, Relevance\n",
    "```\n",
    "\n",
    "### Структурные токены\n",
    "\n",
    "Для того чтобы LLM чётко разделяла **контекст**, **вопрос** и **инструкцию**, мы используем\n",
    "XML-подобные токены-разделители:\n",
    "\n",
    "| Токен | Назначение |\n",
    "|-------|-----------|\n",
    "| `<CONTEXT>` / `</CONTEXT>` | Обрамляет весь блок найденных источников |\n",
    "| `<SOURCE pmid=\"...\" section=\"...\" year=\"...\">` / `</SOURCE>` | Каждый отдельный чанк с метаданными |\n",
    "| `<QUESTION>` / `</QUESTION>` | Вопрос пользователя |\n",
    "| `<INSTRUCTION>` / `</INSTRUCTION>` | Блок инструкций для модели |\n",
    "\n",
    "Модель обучена работать с XML-тегами и может надёжно различать, где заканчивается контекст\n",
    "и начинается вопрос. Это значительно снижает **hallucination** и повышает **faithfulness**.\n",
    "\n",
    "| Компонент | Подход | Зачем |\n",
    "|-----------|--------|-------|\n",
    "| Hybrid search | BM25 + dense + RRF | BM25 ловит точные термины (BACE1, TREM2), dense — семантику |\n",
    "| Cross-encoder | `ms-marco-MiniLM-L-6-v2` | Переранжирование кандидатов — +20-40% precision@k |\n",
    "| Structured prompt | XML-токены | Модель различает контекст/вопрос → выше Faithfulness |\n",
    "| Top-K | Настраиваемый (по умолчанию 5/7) | Баланс между полнотой и шумом |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212fc84",
   "metadata": {},
   "source": [
    "### 4.1 Выбор модели и настройки\n",
    "\n",
    "Интерактивный каталог моделей. `trust_remote_code=True` требуется для некоторых архитектур\n",
    "(например, Qwen использует собственный attention). В продакшене следует проверять\n",
    "источник модели на HuggingFace перед использованием этого флага."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ff345",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CATALOG = OrderedDict({\n",
    "    'TinyLlama/TinyLlama-1.1B-Chat-v1.0': {\n",
    "        'label': 'TinyLlama 1.1B — самая лёгкая (CPU ~5 GB)',\n",
    "        'size': '1.1B', 'vram_fp16': '~3 GB', 'vram_4bit': '~1.5 GB',\n",
    "        'ram_cpu': '~5 GB', 'quality': '2/5', 'speed': '5/5',\n",
    "        'license': 'Apache 2.0', 'gated': False,\n",
    "        'notes': 'Для слабых машин. Низкое качество цитирования.',\n",
    "    },\n",
    "    'Qwen/Qwen2-1.5B-Instruct': {\n",
    "        'label': 'Qwen2 1.5B — рекомендуется (CPU ~7 GB)',\n",
    "        'size': '1.5B', 'vram_fp16': '~4 GB', 'vram_4bit': '~2 GB',\n",
    "        'ram_cpu': '~7 GB', 'quality': '3/5', 'speed': '5/5',\n",
    "        'license': 'Apache 2.0', 'gated': False,\n",
    "        'notes': 'Лучший для CPU. Приемлемое цитирование.',\n",
    "    },\n",
    "    'microsoft/phi-2': {\n",
    "        'label': 'Phi-2 2.7B — reasoning (CPU ~11 GB)',\n",
    "        'size': '2.7B', 'vram_fp16': '~6 GB', 'vram_4bit': '~3 GB',\n",
    "        'ram_cpu': '~11 GB', 'quality': '3/5', 'speed': '4/5',\n",
    "        'license': 'MIT', 'gated': False,\n",
    "        'notes': 'Нет chat template (plain prompt).',\n",
    "    },\n",
    "    'google/gemma-2-2b-it': {\n",
    "        'label': 'Gemma 2 2B (gated, нужен HF token)',\n",
    "        'size': '2.6B', 'vram_fp16': '~6 GB', 'vram_4bit': '~3 GB',\n",
    "        'ram_cpu': '~11 GB', 'quality': '3/5', 'speed': '4/5',\n",
    "        'license': 'Gemma License', 'gated': True,\n",
    "        'notes': 'Нужен HF token.',\n",
    "    },\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3': {\n",
    "        'label': 'Mistral 7B — лучший баланс (GPU 8+ GB)',\n",
    "        'size': '7.2B', 'vram_fp16': '~15 GB', 'vram_4bit': '~5 GB',\n",
    "        'ram_cpu': '~30 GB', 'quality': '4/5', 'speed': '3/5',\n",
    "        'license': 'Apache 2.0', 'gated': False,\n",
    "        'notes': 'Рекомендуется 4-bit на GPU 8+ GB.',\n",
    "    },\n",
    "    'Qwen/Qwen2-7B-Instruct': {\n",
    "        'label': 'Qwen2 7B (GPU 8+ GB)',\n",
    "        'size': '7.6B', 'vram_fp16': '~16 GB', 'vram_4bit': '~5 GB',\n",
    "        'ram_cpu': '~32 GB', 'quality': '4/5', 'speed': '3/5',\n",
    "        'license': 'Apache 2.0', 'gated': False,\n",
    "        'notes': 'Отличное следование инструкциям.',\n",
    "    },\n",
    "    'meta-llama/Meta-Llama-3.1-8B-Instruct': {\n",
    "        'label': 'Llama 3.1 8B — лучшее качество (gated, GPU)',\n",
    "        'size': '8.0B', 'vram_fp16': '~17 GB', 'vram_4bit': '~6 GB',\n",
    "        'ram_cpu': '~34 GB', 'quality': '5/5', 'speed': '2/5',\n",
    "        'license': 'Llama 3.1 Community', 'gated': True,\n",
    "        'notes': 'Нужен HF token. Лучшая faithfulness.',\n",
    "    },\n",
    "})\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[(v['label'], k) for k, v in MODEL_CATALOG.items()],\n",
    "    value='Qwen/Qwen2-1.5B-Instruct', description='Модель:',\n",
    "    style={'description_width': '80px'}, layout=widgets.Layout(width='100%'))\n",
    "quantize_radio = widgets.RadioButtons(\n",
    "    options=[('FP16/FP32','none'), ('4-bit NF4','4bit'), ('8-bit','8bit')],\n",
    "    value='none', description='Квант.:',\n",
    "    style={'description_width': '80px'}, layout=widgets.Layout(width='100%'))\n",
    "token_input = widgets.Password(value='', placeholder='hf_XXXX (для gated моделей)',\n",
    "    description='HF Token:', style={'description_width': '80px'},\n",
    "    layout=widgets.Layout(width='100%'))\n",
    "topk_slider = widgets.IntSlider(value=3, min=1, max=15, description='Top-K:',\n",
    "    style={'description_width': '50px'}, layout=widgets.Layout(width='200px'))\n",
    "max_tokens_slider = widgets.IntSlider(value=512, min=128, max=2048, step=128,\n",
    "    description='Max tok:', style={'description_width': '60px'},\n",
    "    layout=widgets.Layout(width='280px'))\n",
    "\n",
    "model_info = widgets.Output()\n",
    "def show_info(change=None):\n",
    "    model_info.clear_output()\n",
    "    with model_info:\n",
    "        info = MODEL_CATALOG[model_dropdown.value]\n",
    "        print(f'  {model_dropdown.value}')\n",
    "        print(f'  Размер: {info[\"size\"]} | VRAM: {info[\"vram_4bit\"]} (4bit) | CPU RAM: {info[\"ram_cpu\"]}')\n",
    "        print(f'  Качество: {info[\"quality\"]} | Скорость: {info[\"speed\"]} | {info[\"notes\"]}')\n",
    "        if info['gated'] and not token_input.value:\n",
    "            print(f'  [!] Gated модель — введите HF Token!')\n",
    "model_dropdown.observe(show_info, names='value'); show_info()\n",
    "\n",
    "display(widgets.HTML('<h3>Настройки LLM</h3>'))\n",
    "display(model_dropdown); display(model_info)\n",
    "display(quantize_radio); display(token_input)\n",
    "display(widgets.HBox([topk_slider, max_tokens_slider]))\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SIMILARITY_THRESHOLD = 0.25\n",
    "TEMPERATURE = 0.3\n",
    "TOP_P = 0.9\n",
    "REPETITION_PENALTY = 1.12\n",
    "MAX_CONTEXT_LENGTH = 4096\n",
    "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "print(f'\\nDevice: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11272137",
   "metadata": {},
   "source": [
    "### 4.2 Применение настроек\n",
    "\n",
    "Фиксируем выбранные пользователем параметры из виджетов.\n",
    "После выполнения этой ячейки все последующие шаги используют `HF_MODEL`, `TOP_K` и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL = model_dropdown.value\n",
    "QUANTIZE = quantize_radio.value\n",
    "HF_TOKEN = token_input.value or None\n",
    "TOP_K = topk_slider.value\n",
    "MAX_NEW_TOKENS = max_tokens_slider.value\n",
    "\n",
    "info = MODEL_CATALOG[HF_MODEL]\n",
    "print(f'Модель: {HF_MODEL} ({info[\"size\"]}) | quant={QUANTIZE} | top_k={TOP_K}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562131c",
   "metadata": {},
   "source": [
    "### 4.3 Cross-encoder Re-ranker\n",
    "\n",
    "Cross-encoder принимает пару `(query, document)` и выдаёт единый скор релевантности.\n",
    "В отличие от bi-encoder (SentenceTransformer), cross-encoder моделирует **взаимодействие**\n",
    "между запросом и документом через full attention, что даёт значительно более точное\n",
    "ранжирование (+20-40% precision@k).\n",
    "\n",
    "Используем `ms-marco-MiniLM-L-6-v2` — компактную модель, обученную на MS MARCO.\n",
    "Она принимает кандидатов из hybrid search и переранжирует их по релевантности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7ef91",
   "metadata": {},
   "source": [
    "### `rerank`\n",
    "\n",
    "Переранжирует кандидатов из hybrid search с помощью cross-encoder (ms-marco-MiniLM).\n",
    "\n",
    "**Вход:** `query` (str), `chunks_list` (list[dict]) — кандидаты, `top_n` (int)  \n",
    "**Выход:** `list[dict]` — top_n чанков, отсортированных по rerank_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87496293",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(RERANKER_MODEL, max_length=512)\n",
    "print(f'Re-ranker: {RERANKER_MODEL}')\n",
    "\n",
    "def rerank(query: str, chunks_list: list, top_n: int = 5) -> list:\n",
    "    if not chunks_list:\n",
    "        return []\n",
    "    pairs = [(query, c['text']) for c in chunks_list]\n",
    "    scores = reranker.predict(pairs)\n",
    "    for c, s in zip(chunks_list, scores):\n",
    "        c['rerank_score'] = float(s)\n",
    "    return sorted(chunks_list, key=lambda x: x['rerank_score'], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154a9a1",
   "metadata": {},
   "source": [
    "### 4.4 Структурные токены для разделения контекста, вопроса и инструкций\n",
    "\n",
    "**Проблема**: маленькие LLM (1-3B) часто путают контекст с инструкцией — галлюцинируют\n",
    "или игнорируют найденные источники.\n",
    "\n",
    "**Решение**: XML-подобные токены-разделители. Модели (особенно Qwen, Llama, Mistral)\n",
    "обучены на данных с XML/HTML-разметкой и хорошо распознают структурные теги.\n",
    "\n",
    "| Токен | Где используется | Зачем |\n",
    "|-------|-----------------|-------|\n",
    "| `<CONTEXT>` / `</CONTEXT>` | Оборачивает все найденные чанки | Модель понимает: «всё внутри — это мои источники» |\n",
    "| `<SOURCE pmid=\"...\" ...>` / `</SOURCE>` | Каждый отдельный чанк | Модель может точно цитировать `[PMID:X]` |\n",
    "| `<QUESTION>` / `</QUESTION>` | Вопрос пользователя | Чёткая граница: контекст закончился, вот вопрос |\n",
    "| `<INSTRUCTION>` / `</INSTRUCTION>` | Правила генерации | Модель знает: это не контент, а управляющие указания |\n",
    "\n",
    "Эти токены **упоминаются в system prompt**, чтобы модель заранее знала формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_OPEN  = \"<CONTEXT>\"\n",
    "CONTEXT_CLOSE = \"</CONTEXT>\"\n",
    "SOURCE_OPEN   = \"<SOURCE\"\n",
    "SOURCE_CLOSE  = \"</SOURCE>\"\n",
    "QUESTION_OPEN  = \"<QUESTION>\"\n",
    "QUESTION_CLOSE = \"</QUESTION>\"\n",
    "INSTRUCTION_OPEN  = \"<INSTRUCTION>\"\n",
    "INSTRUCTION_CLOSE = \"</INSTRUCTION>\"\n",
    "\n",
    "print(\"Структурные токены определены:\")\n",
    "print(f\"  Контекст:   {CONTEXT_OPEN} ... {CONTEXT_CLOSE}\")\n",
    "print(f\"  Источник:   {SOURCE_OPEN} pmid=\\\"...\\\" section=\\\"...\\\" year=\\\"...\\\">{SOURCE_CLOSE}\")\n",
    "print(f\"  Вопрос:     {QUESTION_OPEN} ... {QUESTION_CLOSE}\")\n",
    "print(f\"  Инструкция: {INSTRUCTION_OPEN} ... {INSTRUCTION_CLOSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71994cf",
   "metadata": {},
   "source": [
    "### 4.5 System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"### ROLE\\n\"\n",
    "    \"You are a biomedical research assistant specializing in Alzheimer's disease drug target discovery. \"\n",
    "    \"Your sole purpose is to answer research questions using ONLY the provided scientific sources.\\n\\n\"\n",
    "    \n",
    "    \"### INPUT FORMAT\\n\"\n",
    "    \"- <CONTEXT>...</CONTEXT>: Contains retrieved source chunks. THIS IS YOUR ONLY KNOWLEDGE BASE.\\n\"\n",
    "    \"- <SOURCE pmid='X' section='Y'>...</SOURCE>: Individual evidence unit with PubMed ID and section label.\\n\"\n",
    "    \"- <QUESTION>...</QUESTION>: The user's research query.\\n\\n\"\n",
    "    \n",
    "    \"### MANDATORY RULES\\n\"\n",
    "    \"1. SOURCE-ONLY: Answer STRICTLY using information present in <SOURCE> chunks. NO external knowledge, NO inference beyond text.\\n\"\n",
    "    \"2. CITATION FORMAT: Append [PMID:X] IMMEDIATELY AFTER each factual claim, BEFORE punctuation. If PMID missing → [Source:section_id].\\n\"\n",
    "    \"3. INSUFFICIENT DATA: If <CONTEXT> lacks information to answer → respond EXACTLY: 'Insufficient data in provided sources.'\\n\"\n",
    "    \"4. LANGUAGE: Match the language of the <QUESTION> exactly.\\n\"\n",
    "    \"5. CONCISENESS: Max 250 words. NO introductions ('Based on...', 'According to...'). Use cautious academic language: 'reported', 'suggests', 'may indicate'.\\n\"\n",
    "    \"6. CONFLICT RESOLUTION: If sources contradict → state both: 'Source A reports X [PMID:A], while Source B suggests Y [PMID:B]'.\\n\"\n",
    "    \"7. NO SPECULATION: Never extrapolate, generalize, or fill gaps. If evidence is partial, state limitations explicitly.\\n\\n\"\n",
    "    \n",
    "    \"### OUTPUT VALIDATION (self-check before responding)\\n\"\n",
    "    \"✓ Every claim has a valid [PMID:X] or [Source:section_id]?\\n\"\n",
    "    \"✓ All PMIDs/section_ids exist in <CONTEXT>?\\n\"\n",
    "    \"✓ Zero external knowledge used?\\n\"\n",
    "    \"✓ Response language matches <QUESTION>?\\n\"\n",
    "    \"✓ Word count ≤250 and no introductory phrases?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf43387",
   "metadata": {},
   "source": [
    "### 4.6 Вспомогательная функция: cosine similarity\n",
    "\n",
    "Используется в MMR (Maximal Marginal Relevance) для оценки разнообразия\n",
    "выбранных чанков. Реализация на чистом Python без numpy для совместимости.\n",
    "При получении векторов разной длины выводит предупреждение (`warnings.warn`) и\n",
    "обрезает до минимальной длины — это сигнал об ошибке выше по pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61087c71",
   "metadata": {},
   "source": [
    "### `cosine_similarity`\n",
    "\n",
    "Вычисляет косинусное сходство двух векторов (чистый Python, без numpy).\n",
    "Если длины не совпадают — выводит `warnings.warn` и обрезает до минимальной длины.\n",
    "\n",
    "**Вход:** `vec1` (list[float]), `vec2` (list[float])  \n",
    "**Выход:** `float` — значение cosine similarity в диапазоне [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe68310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: list, vec2: list):\n",
    "    if not vec1 or not vec2:\n",
    "        return 0.0\n",
    "    if len(vec1) != len(vec2):\n",
    "        warnings.warn(f'cosine_similarity: vectors have different lengths ({len(vec1)} vs {len(vec2)}), truncating to min length')\n",
    "        min_len = min(len(vec1), len(vec2))\n",
    "        vec1, vec2 = vec1[:min_len], vec2[:min_len]\n",
    "    dot = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    norm2 = math.sqrt(sum(b * b for b in vec2))\n",
    "    if norm1 * norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df93d15",
   "metadata": {},
   "source": [
    "### 4.7 Maximal Marginal Relevance (MMR)\n",
    "\n",
    "MMR балансирует **релевантность** (score к запросу) и **разнообразие** (непохожесть на уже выбранные).\n",
    "Параметр `diversity` управляет балансом:\n",
    "- `diversity=1.0` — чистая релевантность (жадный top-K)\n",
    "- `diversity=0.5` — баланс (рекомендуется)\n",
    "- `diversity=0.0` — максимальное разнообразие\n",
    "\n",
    "Используется как fallback, когда cross-encoder re-ranking отключён.\n",
    "Перед вызовом MMR функция `retrieve` вычисляет эмбеддинги кандидатов\n",
    "через `encoder.encode()` и записывает их в поле `embedding` — это необходимо\n",
    "для корректного расчёта `cosine_similarity` между кандидатами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5ca10",
   "metadata": {},
   "source": [
    "### `apply_mmr`\n",
    "\n",
    "Maximal Marginal Relevance — выбирает разнообразное подмножество чанков, балансируя релевантность и непохожесть.\n",
    "\n",
    "**Вход:** `candidates` (list[dict]), `query` (str), `k` (int), `diversity` (float 0..1)  \n",
    "**Выход:** `list[dict]` — k отобранных чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mmr(candidates: list, query: str, k: int = 5, diversity: float = 0.5) -> list:\n",
    "    if len(candidates) <= k:\n",
    "        return candidates\n",
    "\n",
    "    selected = []\n",
    "    pool = sorted(candidates, key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    while len(selected) < k and pool:\n",
    "        if not selected:\n",
    "            selected.append(pool.pop(0))\n",
    "        else:\n",
    "            best_score = -float('inf')\n",
    "            best_idx = -1\n",
    "            for i, doc in enumerate(pool):\n",
    "                rel = doc.get('score', 0)\n",
    "\n",
    "                if doc.get('embedding') and selected:\n",
    "                    raw_sim = max(\n",
    "                        cosine_similarity(doc['embedding'], sel.get('embedding', []))\n",
    "                        for sel in selected\n",
    "                    )\n",
    "                    sim_to_selected = (raw_sim + 1) / 2\n",
    "                else:\n",
    "                    sim_to_selected = 0\n",
    "\n",
    "                mmr_score = diversity * rel - (1 - diversity) * sim_to_selected\n",
    "                if mmr_score > best_score:\n",
    "                    best_score = mmr_score\n",
    "                    best_idx = i\n",
    "\n",
    "            if best_idx >= 0:\n",
    "                selected.append(pool.pop(best_idx))\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f25f11",
   "metadata": {},
   "source": [
    "### 4.8 Сборка промпта с XML-токенами (`build_prompt`)\n",
    "\n",
    "Функция собирает финальный промпт для LLM. Структура:\n",
    "\n",
    "```\n",
    "[System message]  ← SYSTEM_PROMPT (правила генерации)\n",
    "\n",
    "[User message]:\n",
    "<CONTEXT>\n",
    "  <SOURCE pmid=\"12345\" section=\"abstract\" year=\"2023\">\n",
    "    Текст чанка 1...\n",
    "  </SOURCE>\n",
    "  <SOURCE pmid=\"67890\" section=\"introduction\" year=\"2022\">\n",
    "    Текст чанка 2...\n",
    "  </SOURCE>\n",
    "</CONTEXT>\n",
    "\n",
    "<QUESTION>Вопрос пользователя</QUESTION>\n",
    "\n",
    "<INSTRUCTION>\n",
    "Правила генерации ответа...\n",
    "</INSTRUCTION>\n",
    "```\n",
    "\n",
    "**Логика**:\n",
    "1. **Бюджет токенов** — вычисляется `available = max_context_tokens - overhead - reserve_tokens`\n",
    "2. **Валидация PMID** — проверяется, что PMID состоит из цифр; невалидные заменяются на `\"none\"`\n",
    "3. **XML-экранирование** — спецсимволы в тексте (`&`, `<`, `>`) экранируются через `_escape_xml`\n",
    "4. **Лимит токенов** — чанки добавляются последовательно, пока не исчерпан бюджет\n",
    "5. **Fallback** — если ни один чанк не прошёл, вставляется `\"No sources retrieved.\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d0c63",
   "metadata": {},
   "source": [
    "### `count_tokens`, `_escape_xml`, `build_prompt`\n",
    "\n",
    "- `count_tokens` — считает количество токенов в тексте. Сначала пытается использовать токенизатор загруженной модели (`tokenizer.encode`); если модель ещё не загружена — использует tiktoken `cl100k_base` как fallback. **Вход:** str. **Выход:** int\n",
    "- `_escape_xml` — экранирует спецсимволы XML (&, <, >) в тексте чанков. **Вход:** str. **Выход:** str\n",
    "- `build_prompt` — собирает финальный промпт в формате messages с XML-токенами (CONTEXT, SOURCE, QUESTION, INSTRUCTION). Валидирует PMID, экранирует XML, контролирует бюджет токенов. **Вход:** query (str), chunks_list (list[dict]), max_context_tokens, reserve_tokens, debug. **Выход:** list[dict] — messages [{role, content}, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f82930",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_ENCODER = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    try:\n",
    "        if 'tokenizer' in globals() and tokenizer is not None:\n",
    "            return len(tokenizer.encode(text))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return len(TOKEN_ENCODER.encode(text))\n",
    "\n",
    "\n",
    "def _escape_xml(text: str) -> str:\n",
    "    return (text\n",
    "            .replace(\"&\", \"&amp;\")\n",
    "            .replace(\"<\", \"&lt;\")\n",
    "            .replace(\">\", \"&gt;\"))\n",
    "\n",
    "\n",
    "def build_prompt(\n",
    "    query: str,\n",
    "    chunks_list: list[dict],\n",
    "    max_context_tokens: int = 3000,\n",
    "    reserve_tokens: int = 500,\n",
    "    debug: bool = False\n",
    ") -> list[dict]:\n",
    "    context_parts: list[str] = []\n",
    "    used_tokens = 0\n",
    "\n",
    "    structure_template = \"<CONTEXT>\\n\\n</CONTEXT>\\n\\n<QUESTION></QUESTION>\\n\\n<INSTRUCTION>\\n\\n</INSTRUCTION>\"\n",
    "\n",
    "    instruction = (\n",
    "        \"Answer <QUESTION> using ONLY <CONTEXT>. Rules:\\n\"\n",
    "        \"1) Cite EVERY claim as '...text [PMID:X].' (BEFORE period). Multiple: [PMID:A][PMID:B].\\n\"\n",
    "        \"2) If context lacks info on part of question: state '[PMID:N/A] No data on [topic]'.\\n\"\n",
    "        \"3) NO repetition: cite each source once per unique point.\\n\"\n",
    "        \"4) Structure: 1-sentence summary → bullet points → brief synthesis.\\n\"\n",
    "        \"5) Note conflicts: '[PMID:A] says X, [PMID:B] suggests Y'.\\n\"\n",
    "        \"6) Answer language = question language. Keep PMIDs/technical terms unchanged.\\n\\n\"\n",
    "        \"<ANSWER>:\"\n",
    "    )\n",
    "    \n",
    "    overhead = count_tokens(structure_template + \"\\n\\n\" + instruction_template)\n",
    "    available = max_context_tokens - overhead - reserve_tokens\n",
    "\n",
    "    for chunk in chunks_list:\n",
    "        pmid = chunk.get(\"pmid\") or chunk.get(\"meta\", {}).get(\"pmid\") or \"none\"\n",
    "        section = chunk.get(\"section\") or chunk.get(\"meta\", {}).get(\"section\") or \"unknown\"\n",
    "        year = chunk.get(\"year\") or chunk.get(\"meta\", {}).get(\"year\") or \"\"\n",
    "        text = str(chunk.get(\"text\", \"\")).strip()\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        pmid_str = str(pmid).strip()\n",
    "        if pmid_str and pmid_str.lower() not in (\"none\", \"unknown\", \"null\", \"\"):\n",
    "            if pmid_str.isdigit():\n",
    "                pmid_clean = pmid_str\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"Invalid PMID '{pmid_str}' -> replaced with 'none'\")\n",
    "                pmid_clean = \"none\"\n",
    "        else:\n",
    "            pmid_clean = \"none\"\n",
    "\n",
    "        text_escaped = _escape_xml(text)\n",
    "\n",
    "        year_str = str(year).strip() if year else \"\"\n",
    "        source_tag = f'<SOURCE pmid=\"{pmid_clean}\" section=\"{section}\" year=\"{year_str}\">'\n",
    "        source_block = f\"{source_tag}{text_escaped}</SOURCE>\"\n",
    "\n",
    "        chunk_tokens = count_tokens(source_block) + 10\n",
    "        if used_tokens + chunk_tokens > available:\n",
    "            if debug:\n",
    "                print(f\"Context limit reached after {len(context_parts)} chunks\")\n",
    "            break\n",
    "\n",
    "        context_parts.append(source_block)\n",
    "        used_tokens += chunk_tokens\n",
    "\n",
    "    if context_parts:\n",
    "        context_block = \"\\n\".join(context_parts)\n",
    "    else:\n",
    "        context_block = '<SOURCE pmid=\"none\" section=\"none\" year=\"\">No sources retrieved.</SOURCE>'\n",
    "\n",
    "    user_content = (\n",
    "        f\"<CONTEXT>\\n{context_block}\\n</CONTEXT>\\n\\n\"\n",
    "        f\"<QUESTION>{query}</QUESTION>\\n\\n\"\n",
    "        f\"<INSTRUCTION>{instruction}</INSTRUCTION>\"\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        cited_in_context = re.findall(r'pmid=\"([^\"]+)\"', context_block)\n",
    "        print(f\"DEBUG: PMIDs in context: {set(cited_in_context)}\")\n",
    "        print(f\"DEBUG: Context tokens: {used_tokens}/{available}\")\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68492a0",
   "metadata": {},
   "source": [
    "### Пример: генерация контекста и промпта\n",
    "\n",
    "Демонстрация того, как `retrieve` формирует контекст, а `build_prompt` собирает финальный промпт для LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"What role does TREM2 play as a therapeutic target in Alzheimer's disease?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ПРИМЕР: Генерация контекста и промпта\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nЗапрос: {example_query}\")\n",
    "\n",
    "print(\"\\n--- Шаг 1: Hybrid search (children) ---\")\n",
    "child_res = search_hybrid_children(example_query, top_k=10)\n",
    "print(f\"Найдено children: {len(child_res)}\")\n",
    "for cr in child_res[:3]:\n",
    "    print(f\"  rrf={cr['rrf_score']:.4f} | {cr['text'][:80]}...\")\n",
    "\n",
    "print(\"\\n--- Шаг 2: Агрегация children → parents ---\")\n",
    "parent_res = children_to_parents(child_res, max_parents=5)\n",
    "print(f\"Уникальных parents: {len(parent_res)}\")\n",
    "for j, p in enumerate(parent_res, 1):\n",
    "    print(f\"  [{j}] PMID:{p['pmid']} | {p['section']} | sim={p['similarity']:.3f} | {len(p['text'])} симв.\")\n",
    "\n",
    "print(\"\\n--- Шаг 3: Сборка промпта (build_prompt) ---\")\n",
    "messages = build_prompt(example_query, parent_res, debug=True)\n",
    "\n",
    "print(f\"\\nСообщений в промпте: {len(messages)}\")\n",
    "print(f\"  system: {len(messages[0]['content'])} символов\")\n",
    "print(f\"  user:   {len(messages[1]['content'])} символов\")\n",
    "\n",
    "print(\"\\n--- System prompt ---\")\n",
    "print(messages[0]['content'])\n",
    "\n",
    "print(\"\\n--- User content ---\")\n",
    "print(messages[1]['content'])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044151ba",
   "metadata": {},
   "source": [
    "### 4.9 Основная функция retrieval (`retrieve`)\n",
    "\n",
    "Оркестрирует весь pipeline поиска:\n",
    "\n",
    "1. **Hybrid search** по children (dense + BM25 + RRF fusion) или только dense\n",
    "2. **Child → Parent** — агрегация мелких чанков в полные секции\n",
    "3. **Фильтрация** по порогу similarity (SIMILARITY_THRESHOLD = 0.3)\n",
    "4. **Нормализация** scores в диапазон [0, 1] (min-max)\n",
    "5. **Fallback** — если после фильтрации не осталось кандидатов, берутся top-3 child_results с извлечением метаданных из `meta`\n",
    "6. **Re-ranking** (cross-encoder) или **MMR** с предварительным вычислением эмбеддингов кандидатов\n",
    "7. **Валидация** метаданных — проверка обязательных полей (pmid, title, journal, year, section, text), приведение year к int\n",
    "\n",
    "Параметры `use_hybrid` и `use_rerank` управляются из интерфейса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b3f21",
   "metadata": {},
   "source": [
    "### `retrieve`\n",
    "\n",
    "Оркестрирует полный pipeline поиска: hybrid search → parent aggregation → фильтрация → нормализация → fallback при пустых кандидатах → re-ranking/MMR (с вычислением эмбеддингов) → валидация метаданных.\n",
    "\n",
    "**Вход:** `query` (str), `top_k` (int), `use_hybrid` (bool), `use_rerank` (bool)  \n",
    "**Выход:** `list[dict]` — top_k чанков с метаданными, готовых для build_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5, use_hybrid: bool = True, use_rerank: bool = True) -> list:\n",
    "    SIMILARITY_THRESHOLD = 0.3\n",
    "    children_k = top_k * 5\n",
    "\n",
    "    if use_hybrid:\n",
    "        child_results = search_hybrid_children(\n",
    "            query, top_k=children_k, dense_k=children_k, bm25_k=children_k\n",
    "        )\n",
    "    else:\n",
    "        dense = search_dense(query, top_k=children_k)\n",
    "        child_results = [\n",
    "            {'child_id': d['id'], 'score': d.get('sim', d.get('rrf_score', 0.0)), **d}\n",
    "            for d in dense\n",
    "        ]\n",
    "\n",
    "    candidates = children_to_parents(child_results, max_parents=top_k * 3)\n",
    "\n",
    "    for c in candidates:\n",
    "        c['score'] = c.get('score', c.get('similarity', c.get('rrf_score', 0.0)))\n",
    "\n",
    "    candidates = [c for c in candidates if c.get('score', 0.0) >= SIMILARITY_THRESHOLD]\n",
    "\n",
    "    if candidates:\n",
    "        scores = [c['score'] for c in candidates]\n",
    "        min_s, max_s = min(scores), max(scores)\n",
    "        if max_s > min_s:\n",
    "            for c in candidates:\n",
    "                c['score'] = (c['score'] - min_s) / (max_s - min_s)\n",
    "\n",
    "    if not candidates:\n",
    "        fallback = sorted(child_results, key=lambda x: x.get('score', x.get('rrf_score', 0)), reverse=True)[:3]\n",
    "        for fb in fallback:\n",
    "            meta = fb.get('meta', {})\n",
    "            candidates.append({\n",
    "                'pmid': meta.get('pmid', ''),\n",
    "                'title': meta.get('title', ''),\n",
    "                'journal': meta.get('journal', ''),\n",
    "                'year': meta.get('year', ''),\n",
    "                'section': meta.get('section', ''),\n",
    "                'text': fb.get('text', ''),\n",
    "                'similarity': fb.get('score', fb.get('rrf_score', 0)),\n",
    "                'score': fb.get('score', fb.get('rrf_score', 0)),\n",
    "            })\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "    if use_rerank and len(candidates) > 1:\n",
    "        candidates = rerank(query, candidates, top_n=top_k)\n",
    "    else:\n",
    "        cand_texts = [c.get('text', '') for c in candidates]\n",
    "        if cand_texts:\n",
    "            cand_embs = encoder.encode(cand_texts, normalize_embeddings=True).tolist()\n",
    "            for c, emb in zip(candidates, cand_embs):\n",
    "                c['embedding'] = emb\n",
    "        candidates = apply_mmr(candidates, query, k=top_k, diversity=0.5)\n",
    "\n",
    "    cleaned = []\n",
    "    required_fields = ['pmid', 'title', 'journal', 'year', 'section', 'text']\n",
    "\n",
    "    for c in candidates[:top_k]:\n",
    "        text = c.get('text', '')\n",
    "        if not text or len(str(text).strip()) < 20:\n",
    "            continue\n",
    "\n",
    "        y = c.get('year')\n",
    "        if y:\n",
    "            try:\n",
    "                c['year'] = int(y)\n",
    "            except (ValueError, TypeError):\n",
    "                c['year'] = 0\n",
    "        else:\n",
    "            c['year'] = 0\n",
    "\n",
    "        for field in required_fields:\n",
    "            if field == 'year':\n",
    "                continue\n",
    "            if field not in c or not c[field]:\n",
    "                c[field] = None if field == 'pmid' else 'Unknown'\n",
    "\n",
    "        if c.get('pmid') and not str(c['pmid']).isdigit():\n",
    "            c['pmid'] = None\n",
    "\n",
    "        cleaned.append(c)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267103ae",
   "metadata": {},
   "source": [
    "### 4.10 Загрузка LLM\n",
    "\n",
    "`trust_remote_code=True` необходим для архитектур Qwen и некоторых других моделей,\n",
    "которые регистрируют собственные классы. Используйте только модели из проверенных\n",
    "источников (официальные организации на HuggingFace).\n",
    "\n",
    "Поддерживается:\n",
    "- **FP16** — для GPU с достаточной VRAM\n",
    "- **4-bit NF4** — квантизация через bitsandbytes (GPU only)\n",
    "- **8-bit** — промежуточный вариант (GPU only)\n",
    "- **FP32** — для CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a397253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tk = {'trust_remote_code': True}\n",
    "lk = {'trust_remote_code': True}\n",
    "if HF_TOKEN:\n",
    "    tk['token'] = HF_TOKEN\n",
    "    lk['token'] = HF_TOKEN\n",
    "\n",
    "print(f'Загрузка: {HF_MODEL}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, **tk)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if QUANTIZE == '4bit' and DEVICE == 'cuda':\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    lk['quantization_config'] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True)\n",
    "    lk['device_map'] = 'auto'\n",
    "elif QUANTIZE == '8bit' and DEVICE == 'cuda':\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    lk['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    lk['device_map'] = 'auto'\n",
    "elif DEVICE == 'cuda':\n",
    "    lk['torch_dtype'] = torch.float16\n",
    "    lk['device_map'] = 'auto'\n",
    "else:\n",
    "    lk['torch_dtype'] = torch.float32\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(HF_MODEL, **lk)\n",
    "if DEVICE == 'cpu' and QUANTIZE == 'none':\n",
    "    llm_model = llm_model.to('cpu')\n",
    "llm_model.eval()\n",
    "\n",
    "has_chat = hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None\n",
    "param_count = sum(p.numel() for p in llm_model.parameters()) / 1e9\n",
    "print(f'Загружено: {HF_MODEL} ({param_count:.2f}B params, chat_template={has_chat})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c714eb",
   "metadata": {},
   "source": [
    "### 4.11 Генерация ответа (`generate_answer`)\n",
    "\n",
    "Формирует промпт через `build_prompt`, передаёт в LLM и выполняет постобработку:\n",
    "\n",
    "1. **Chat template** — если модель поддерживает (Qwen, Llama, Mistral), используем `apply_chat_template`. Результат может быть `Tensor` или `dict` (зависит от версии transformers) — обрабатываются оба варианта через `isinstance` проверку\n",
    "2. **Plain prompt** — для моделей без chat template (Phi-2) используем формат `### System / ### User / ### Answer`\n",
    "3. **Условный sampling** — если `TEMPERATURE > 0`, используется `do_sample=True` с `temperature` и `top_p`; иначе — greedy decoding (`do_sample=False`)\n",
    "4. **Постобработка**:\n",
    "   - Удаление случайно повторённых системных маркеров\n",
    "   - Обнаружение и обрезка зацикливания (>2 одинаковых строки подряд)\n",
    "   - Удаление XML-токенов из ответа (модель иногда копирует их)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66655185",
   "metadata": {},
   "source": [
    "### `generate_answer`\n",
    "\n",
    "Генерирует ответ LLM по найденным чанкам: формирует промпт, обрабатывает результат `apply_chat_template` (Tensor или dict), запускает модель с условным sampling (greedy при TEMPERATURE=0, stochastic при TEMPERATURE>0) и постобрабатывает результат.\n",
    "\n",
    "**Вход:** `query` (str), `chunks_list` (list[dict]), `max_new_tokens` (int)  \n",
    "**Выход:** `str` — текст ответа с цитатами [PMID:X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, chunks_list: list,\n",
    "                    max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    msgs = build_prompt(query, chunks_list)\n",
    "    # print(f\"DEBUG MSGS: {msgs}\") \n",
    "    if has_chat:\n",
    "        chat_out = tokenizer.apply_chat_template(\n",
    "            msgs, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True\n",
    "        )\n",
    "        if isinstance(chat_out, torch.Tensor):\n",
    "            inputs = {\"input_ids\": chat_out}\n",
    "        else:\n",
    "            inputs = chat_out\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"### System:\\n{msgs[0]['content']}\\n\\n\"\n",
    "            f\"### User:\\n{msgs[1]['content']}\\n\\n### Answer:\\n\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    ilen = inputs[\"input_ids\"].shape[1]\n",
    "    max_gen = min(max_new_tokens, tokenizer.model_max_length - ilen - 8)\n",
    "\n",
    "    if max_gen <= 0:\n",
    "        return \"Insufficient data in provided sources.\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_kwargs = dict(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_gen,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        if TEMPERATURE > 0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "        out = llm_model.generate(**gen_kwargs)\n",
    "\n",
    "    ans = tokenizer.decode(out[0][ilen:], skip_special_tokens=True).strip()\n",
    "\n",
    "    for marker in [\"### System:\", \"CONTEXT:\", \"QUESTION:\", \"<CONTEXT>\", \"</CONTEXT>\",\n",
    "                    \"<SOURCE\", \"</SOURCE>\", \"<QUESTION>\", \"</QUESTION>\",\n",
    "                    \"<INSTRUCTION>\", \"</INSTRUCTION>\"]:\n",
    "        if ans.startswith(marker):\n",
    "            ans = ans[len(marker):].strip()\n",
    "\n",
    "    clean = []\n",
    "    prev = None\n",
    "    repeat_count = 0\n",
    "    for line in ans.splitlines():\n",
    "        if line == prev:\n",
    "            repeat_count += 1\n",
    "            if repeat_count > 2:\n",
    "                break\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "        clean.append(line)\n",
    "        prev = line\n",
    "\n",
    "    result = \"\\n\".join(clean).strip()\n",
    "    for tag in [CONTEXT_OPEN, CONTEXT_CLOSE, SOURCE_CLOSE,\n",
    "                QUESTION_OPEN, QUESTION_CLOSE,\n",
    "                INSTRUCTION_OPEN, INSTRUCTION_CLOSE]:\n",
    "        result = result.replace(tag, \"\")\n",
    "    result = re.sub(r'<SOURCE[^>]*>', '', result)\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177dc8b",
   "metadata": {},
   "source": [
    "### 4.12 Proxy-метрики качества (`compute_metrics`)\n",
    "\n",
    "Оценка качества ответа без human evaluation. Все метрики вычисляются автоматически:\n",
    "\n",
    "| Метрика | Что измеряет | Формула |\n",
    "|---------|-------------|---------|\n",
    "| **Faithfulness** | Доля предложений с цитатой `[PMID:X]` | cited_sentences / total_sentences |\n",
    "| **Source Coverage** | Доля PMID из retrieval, процитированных в ответе | cited_pmids ∩ source_pmids / source_pmids |\n",
    "| **Citation Accuracy** | Доля цитат, ссылающихся на реальные источники | cited_pmids ∩ source_pmids / cited_pmids |\n",
    "| **Context Similarity** | Средний retrieval score чанков (cosine sim или RRF) | mean(chunk_similarity) |\n",
    "| **Answer Relevance** | Cosine similarity между эмбеддингами вопроса и ответа | cos_sim(embed(query), embed(answer)) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e31fa",
   "metadata": {},
   "source": [
    "### `compute_metrics`\n",
    "\n",
    "Вычисляет proxy-метрики качества ответа: faithfulness, source coverage, citation accuracy, context similarity, answer relevance.\n",
    "\n",
    "**Вход:** `query` (str), `answer` (str), `chunks_list` (list[dict])  \n",
    "**Выход:** `dict` — словарь метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(query: str, answer: str, chunks_list: list) -> dict:\n",
    "    m = {}\n",
    "\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', answer) if len(s.strip()) > 20]\n",
    "    cited_sentences = sum(1 for s in sentences if re.search(r'\\[PMID:\\d+\\]', s))\n",
    "    m['faithfulness'] = round(cited_sentences / max(len(sentences), 1), 3)\n",
    "\n",
    "    source_pmids = set(str(c.get('pmid', '')) for c in chunks_list if c.get('pmid'))\n",
    "    cited_pmids  = set(re.findall(r'PMID:(\\d+)', answer))\n",
    "    m['source_coverage'] = round(\n",
    "        len(cited_pmids & source_pmids) / max(len(source_pmids), 1), 3\n",
    "    )\n",
    "\n",
    "    m['citation_accuracy'] = round(\n",
    "        len(cited_pmids & source_pmids) / max(len(cited_pmids), 1), 3\n",
    "    )\n",
    "\n",
    "    sims = [c.get('similarity', 0) for c in chunks_list if c.get('similarity')]\n",
    "    m['avg_context_sim'] = round(np.mean(sims), 3) if sims else 0.0\n",
    "\n",
    "    embs = encoder.encode([query, answer], normalize_embeddings=True)\n",
    "    m['answer_relevance'] = round(float(embs[0] @ embs[1]), 3)\n",
    "\n",
    "    m['answer_words'] = len(answer.split())\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b21a20",
   "metadata": {},
   "source": [
    "### 4.13 Пример: полный pipeline retrieve → build_prompt → generate_answer\n",
    "\n",
    "Демонстрация полного цикла от вопроса до сгенерированного ответа с метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_query = \"What are potential targets for Alzheimer's disease treatment?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ПРИМЕР: Полный RAG pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nЗапрос: {demo_query}\")\n",
    "\n",
    "print(\"\\n--- Retrieve ---\")\n",
    "demo_chunks = retrieve(demo_query, top_k=5, use_hybrid=True, use_rerank=True)\n",
    "print(f\"Найдено чанков: {len(demo_chunks)}\")\n",
    "for i, c in enumerate(demo_chunks, 1):\n",
    "    print(f\"  [{i}] PMID:{c.get('pmid','N/A')} | {c.get('section','?')} | \"\n",
    "          f\"score={c.get('rerank_score', c.get('similarity', 0)):.3f} | {len(c.get('text',''))} симв.\")\n",
    "\n",
    "print(\"\\n--- Build Prompt ---\")\n",
    "demo_messages = build_prompt(demo_query, demo_chunks, debug=True)\n",
    "print(f\"\\nSystem prompt: {len(demo_messages[0]['content'])} символов\")\n",
    "print(f\"User content:  {len(demo_messages[1]['content'])} символов\")\n",
    "print(f\"Всего токенов: ~{count_tokens(demo_messages[0]['content']) + count_tokens(demo_messages[1]['content'])}\")\n",
    "\n",
    "print(\"\\n--- user content ---\")\n",
    "print(demo_messages[1]['content'])\n",
    "print(\"...\")\n",
    "\n",
    "print(\"\\n--- Generate Answer ---\")\n",
    "demo_answer = generate_answer(demo_query, demo_chunks)\n",
    "print(demo_answer)\n",
    "\n",
    "print(\"\\n--- Metrics ---\")\n",
    "demo_metrics = compute_metrics(demo_query, demo_answer, demo_chunks)\n",
    "for k, v in demo_metrics.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3209382",
   "metadata": {},
   "source": [
    "### 4.14 Прогон тестовых вопросов\n",
    "\n",
    "Три обязательных вопроса по заданию:\n",
    "1. **Targets** — какие терапевтические мишени описаны в литературе?\n",
    "2. **Druggability** — доступны ли мишени для малых молекул, биологиков или иных модальностей?\n",
    "3. **Research gaps** — какие дополнительные исследования нужны?\n",
    "\n",
    "Для каждого вопроса замеряется время retrieval и generation, вычисляются proxy-метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289aa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTIONS = [\n",
    "    \"What are potential targets for Alzheimer's disease treatment?\",\n",
    "    'Are the targets druggable with small molecules, biologics, or other modalities?',\n",
    "    'What additional studies are needed to advance these targets?',\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for i, q in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f'\\n{\"-\"*60}')\n",
    "    print(f'  Q{i}: {q}')\n",
    "    print('-' * 60)\n",
    "\n",
    "    t0 = time.time()\n",
    "    found = retrieve(q, top_k=TOP_K, use_hybrid=True, use_rerank=True)\n",
    "    ret_ms = (time.time() - t0) * 1000\n",
    "\n",
    "    t1 = time.time()\n",
    "    answer = generate_answer(q, found)\n",
    "    gen_ms = (time.time() - t1) * 1000\n",
    "\n",
    "    m = compute_metrics(q, answer, found)\n",
    "    m['retrieval_ms'] = round(ret_ms, 1)\n",
    "    m['generation_ms'] = round(gen_ms, 1)\n",
    "\n",
    "    all_results.append({\n",
    "        'query': q, 'answer': answer, 'sources': found, 'metrics': m\n",
    "    })\n",
    "\n",
    "    print(f'\\nОтвет: {answer}')\n",
    "    print(f'faith={m[\"faithfulness\"]:.2f} cov={m[\"source_coverage\"]:.2f} '\n",
    "          f'rel={m[\"answer_relevance\"]:.2f} | {ret_ms:.0f}+{gen_ms:.0f}ms')\n",
    "\n",
    "    gc.collect()\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80ee39",
   "metadata": {},
   "source": [
    "### 4.15 Сводка proxy-метрик\n",
    "\n",
    "Агрегированные метрики по всем тестовым вопросам с цветовой индикацией\n",
    "и интерпретацией результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist = [r['metrics'] for r in all_results]\n",
    "mdf = pd.DataFrame(mlist)\n",
    "cols = ['faithfulness','source_coverage','citation_accuracy','avg_context_sim',\n",
    "        'answer_relevance','answer_words','retrieval_ms','generation_ms']\n",
    "summary = mdf[cols].describe().loc[['mean','min','max']].T\n",
    "\n",
    "display(Markdown('## Сводка метрик'))\n",
    "display(summary.style.format('{:.3f}').background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "checks = [\n",
    "    ('faithfulness', 0.7, 0.4, 'Faithfulness',\n",
    "     'хорошо цитирует', 'часть без цитат — нужна более крупная модель', 'нужна 7B+'),\n",
    "    ('source_coverage', 0.6, 0.3, 'Source coverage',\n",
    "     'хорошее покрытие', 'умеренное — увеличьте top_k', 'игнорирует источники'),\n",
    "    ('avg_context_sim', 0.4, 0.3, 'Context relevance',\n",
    "     'релевантные чанки', 'рассмотрите PubMedBERT', 'пересмотрите чанкинг'),\n",
    "    ('answer_relevance', 0.6, 0.4, 'Answer relevance',\n",
    "     'ответы по теме', 'умеренная', 'ответы не по теме'),\n",
    "]\n",
    "print('\\nВЫВОДЫ:')\n",
    "for key, good, ok, name, g, m_, b in checks:\n",
    "    val = mdf[key].mean()\n",
    "    if val >= good:   print(f'  [OK] {name}: {g} ({val:.3f})')\n",
    "    elif val >= ok:   print(f'  [~~] {name}: {m_} ({val:.3f})')\n",
    "    else:             print(f'  [!!] {name}: {b} ({val:.3f})')\n",
    "print(f'\\n  Модель: {HF_MODEL} | Hybrid+Rerank | Top-K={TOP_K}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef84c49",
   "metadata": {},
   "source": [
    "<a id=\"part5\"></a>\n",
    "# Часть 5. Интерактивный интерфейс\n",
    "\n",
    "## Функциональность\n",
    "\n",
    "Интерфейс на `ipywidgets` позволяет исследователю:\n",
    "\n",
    "1. **Ввести произвольный вопрос** или выбрать из примеров\n",
    "2. **Настроить параметры**: Top-K, max tokens, hybrid search, re-ranking\n",
    "3. **Получить ответ** с кликабельными ссылками на PubMed `[PMID:XXXXX]`\n",
    "4. **Просмотреть источники** с оценками релевантности и ссылками\n",
    "5. **Оценить качество** — proxy-метрики для каждого ответа\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_input = widgets.Textarea(\n",
    "    value=\"What are potential targets for Alzheimer's disease treatment?\",\n",
    "    placeholder='Введите вопрос на английском...',\n",
    "    description='Вопрос:',\n",
    "    layout=widgets.Layout(width='100%', height='80px'),\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "examples_dd = widgets.Dropdown(\n",
    "    options=[\n",
    "        '--- Выберите пример ---',\n",
    "        \"What are potential targets for Alzheimer's disease treatment?\",\n",
    "        'Are the targets druggable with small molecules, biologics, or other modalities?',\n",
    "        'What additional studies are needed to advance these targets?',\n",
    "    ],\n",
    "    description='Пример:',\n",
    "    style={'description_width': '60px'},\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "ui_topk = widgets.IntSlider(\n",
    "    value=7, min=1, max=15,\n",
    "    description='Top-K:',\n",
    "    style={'description_width': '50px'},\n",
    "    layout=widgets.Layout(width='170px')\n",
    ")\n",
    "\n",
    "ui_maxtok = widgets.IntSlider(\n",
    "    value=512, min=128, max=2048, step=128,\n",
    "    description='Tokens:',\n",
    "    style={'description_width': '55px'},\n",
    "    layout=widgets.Layout(width='220px')\n",
    ")\n",
    "\n",
    "hybrid_tog = widgets.ToggleButton(\n",
    "    value=True, description='Hybrid search',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='130px')\n",
    ")\n",
    "\n",
    "rerank_tog = widgets.ToggleButton(\n",
    "    value=True, description='Re-ranking',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "search_btn = widgets.Button(\n",
    "    description='Найти',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='120px', height='38px')\n",
    ")\n",
    "\n",
    "out = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        border='1px solid #ccc',\n",
    "        padding='12px',\n",
    "        width='100%'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def on_example_select(change):\n",
    "    if change['new'] != '--- Выберите пример ---':\n",
    "        query_input.value = change['new']\n",
    "\n",
    "examples_dd.observe(on_example_select, names='value')\n",
    "\n",
    "\n",
    "def on_search(b):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        q = query_input.value.strip()\n",
    "        if not q:\n",
    "            display(Markdown('Введите вопрос.'))\n",
    "            return\n",
    "\n",
    "        display(Markdown(f'## Вопрос\\n{q}'))\n",
    "\n",
    "        t0 = time.time()\n",
    "        found = retrieve(\n",
    "            q,\n",
    "            top_k=ui_topk.value,\n",
    "            use_hybrid=hybrid_tog.value,\n",
    "            use_rerank=rerank_tog.value\n",
    "        )\n",
    "        ret_ms = (time.time() - t0) * 1000\n",
    "\n",
    "        if not found:\n",
    "            display(Markdown('Ничего не найдено. Попробуйте переформулировать запрос.'))\n",
    "            return\n",
    "\n",
    "        display(Markdown(f'Найдено источников: **{len(found)}** ({ret_ms:.0f} мс)'))\n",
    "        display(Markdown('*Генерация ответа...*'))\n",
    "\n",
    "        t1 = time.time()\n",
    "        answer = generate_answer(\n",
    "            q,\n",
    "            found,\n",
    "            max_new_tokens=ui_maxtok.value\n",
    "        )\n",
    "        gen_ms = (time.time() - t1) * 1000\n",
    "\n",
    "        display(Markdown('---\\n## Ответ'))\n",
    "        display(Markdown(answer))\n",
    "\n",
    "        display(Markdown(\n",
    "            f'*Время: {ret_ms:.0f} мс (retrieval) + '\n",
    "            f'{gen_ms:.0f} мс (generation) | `{HF_MODEL}`*'\n",
    "        ))\n",
    "\n",
    "        display(Markdown('---\\n## Источники'))\n",
    "\n",
    "        for j, c in enumerate(found, 1):\n",
    "            sim = c.get('similarity', 0.0)\n",
    "            rr = c.get('rerank_score', '')\n",
    "            score_str = f'sim={sim:.3f}'\n",
    "            if rr != '':\n",
    "                score_str += f', rerank={rr:.2f}'\n",
    "\n",
    "            display(Markdown(\n",
    "                f'**[{j}]** {c.get(\"section\",\"\")} '\n",
    "                f'PMID:{c.get(\"pmid\",\"\")} '\n",
    "                f'({score_str}, {c.get(\"year\",\"\")})'\n",
    "            ))\n",
    "\n",
    "            display(Markdown(f'> {c[\"text\"][:200]}...'))\n",
    "\n",
    "        display(Markdown('---\\n## Метрики качества ответа'))\n",
    "        m = compute_metrics(q, answer, found)\n",
    "\n",
    "        for k, v in m.items():\n",
    "            if isinstance(v, float):\n",
    "                status = (\n",
    "                    'хорошо' if v >= 0.6 else\n",
    "                    'умеренно' if v >= 0.3 else\n",
    "                    'низко'\n",
    "                )\n",
    "                display(Markdown(f'- **{k}**: `{v:.3f}` ({status})'))\n",
    "            else:\n",
    "                display(Markdown(f'- **{k}**: `{v}`'))\n",
    "\n",
    "        gc.collect()\n",
    "        if DEVICE == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "search_btn.on_click(on_search)\n",
    "\n",
    "\n",
    "display(widgets.HTML('<h2>Alzheimer Target Discovery — RAG Agent</h2>'))\n",
    "display(examples_dd)\n",
    "display(query_input)\n",
    "display(widgets.HBox([\n",
    "    ui_topk,\n",
    "    ui_maxtok,\n",
    "    hybrid_tog,\n",
    "    rerank_tog,\n",
    "    search_btn\n",
    "]))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316d188",
   "metadata": {},
   "source": [
    "# Теоретические вопросы\n",
    "\n",
    "---\n",
    "\n",
    "## 1. На какие модальности данных можно расширить решение?\n",
    "\n",
    "Текущий pipeline работает только с текстом (абстракты, введения, заключения из PubMed/PMC). Расширение возможно на следующие модальности:\n",
    "\n",
    "**Табличные данные** — таблицы из статей содержат результаты экспериментов: IC50, EC50, Ki значения для ингибиторов, результаты клинических испытаний (p-value, hazard ratio), данные экспрессии генов. Это структурированная информация, которую текущий текстовый RAG теряет при парсинге XML.\n",
    "\n",
    "**Изображения и графики** — гистологические срезы мозга (иммуногистохимия амилоидных бляшек и тау-клубков), графики dose-response, heatmap'ы экспрессии, микрофотографии конфокальной микроскопии. Визуальные данные часто несут информацию, отсутствующую в тексте.\n",
    "\n",
    "**Молекулярные структуры** — 3D-структуры белков-мишеней (PDB), SMILES/InChI представления малых молекул, данные о белок-лигандных взаимодействиях. Критично для вопросов о druggability мишеней.\n",
    "\n",
    "**Геномные и омиксные данные** — последовательности ДНК/РНК, данные GWAS (SNP, ассоциации), протеомика, метаболомика. Хранятся в специализированных форматах (FASTA, VCF, GEO matrix).\n",
    "\n",
    "**Графы знаний** — сети белок-белковых взаимодействий (PPI), сигнальные пути (KEGG, Reactome), онтологии (Gene Ontology). Структура «сущность—связь—сущность» плохо ложится на текущий векторный поиск по тексту в ChromaDB.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Как это можно сделать?\n",
    "\n",
    "### Табличные данные\n",
    "\n",
    "Парсинг таблиц из XML/HTML статей (тег `<table-wrap>` в PMC — аналогично тому, как в текущем pipeline парсятся `<sec>` через `_parse_pmc_xml`). Каждая таблица сериализуется в текст двумя способами: linearization (строка за строкой) для индексации в ChromaDB наравне с текущими child-чанками, и сохранение в структурированном виде (DataFrame) для точных числовых запросов. На этапе retrieval добавляется отдельный Table Retriever — по запросу «IC50 BACE1 inhibitors» он возвращает релевантные строки таблиц. Результаты объединяются с текстовым поиском через RRF (аналогично текущей `search_hybrid_children`). В промпт таблицы вставляются в Markdown-формате внутри отдельного тега `<TABLE_SOURCE>` — расширение текущей XML-разметки `<SOURCE>`.\n",
    "\n",
    "### Изображения и графики\n",
    "\n",
    "Используется vision-language модель (LLaVA, Qwen-VL, GPT-4V через API) для генерации текстовых описаний (caption) каждого рисунка. Полученные описания индексируются как обычные текстовые child-чанки с метаданными `section=figure` и связываются с parent-документом — расширение текущей иерархии parent-child. При retrieval чанки с описаниями рисунков возвращаются наравне с текстовыми и проходят тот же re-ranking через cross-encoder. Для мультимодальной генерации ответа изображение передаётся в VLM вместе с текстовым контекстом.\n",
    "\n",
    "### Молекулярные структуры\n",
    "\n",
    "Интеграция специализированных эмбеддингов: MolBERT / ChemBERTa для SMILES, ESM-2 для белковых последовательностей. Создаётся отдельная коллекция в ChromaDB с молекулярными эмбеддингами (аналогично текущей коллекции `alzheimer_children`, но с другим пространством). Поиск по запросу «small molecule inhibitor of GSK3β» идёт параллельно: текстовый retrieval находит статьи через текущий hybrid search, молекулярный retrieval — конкретные соединения из баз (ChEMBL, PDB). Результаты объединяются через RRF и подаются в промпт.\n",
    "\n",
    "### Геномные данные\n",
    "\n",
    "Подключение внешних API (NCBI Gene, UniProt, STRING) как tool-use для агента — расширение текущего `api_get` хелпера с retry-логикой. Агент сам решает, когда нужно запросить данные об экспрессии гена или PPI-сети. Результаты API-вызовов форматируются в текст и добавляются в `<CONTEXT>` блок промпта. Для GWAS-данных — предварительная индексация ключевых ассоциаций (ген, SNP, p-value, популяция) как структурированных чанков в ChromaDB.\n",
    "\n",
    "### Графы знаний\n",
    "\n",
    "Knowledge Graph Embedding (TransE, RotatE) для сетей взаимодействий. При запросе «upstream regulators of tau phosphorylation» граф обходится от узла TAU по рёбрам phosphorylation/regulation, извлекаются связанные сущности (GSK3β, CDK5, PP2A). Эти сущности используются для query expansion — обогащения поискового запроса перед текущим hybrid search (BM25 + dense). Альтернативно — SPARQL-запросы к Wikidata/UniProt RDF как tool-use.\n",
    "\n",
    "### Общая архитектура мультимодального RAG\n",
    "\n",
    "Расширение текущего pipeline:\n",
    "\n",
    "```\n",
    "Запрос пользователя\n",
    "       |\n",
    "  Query Router (классификатор типа запроса)\n",
    "       |\n",
    "  ┌────┴────┬──────────┬──────────┐\n",
    "  Text    Table    Molecular    KG\n",
    "  Retriever Retriever Retriever  Traversal\n",
    "  (текущий  (новый)  (ChromaDB   (graph\n",
    "  hybrid)           коллекция)  embedding)\n",
    "  │         │         │          │\n",
    "  └────┬────┴─────────┴──────────┘\n",
    "       |\n",
    "  Cross-modal RRF Fusion\n",
    "       |\n",
    "  Cross-Encoder Re-ranking (текущий ms-marco-MiniLM)\n",
    "       |\n",
    "  Multimodal Prompt Assembly (расширение build_prompt)\n",
    "       |\n",
    "  VLM / LLM Generation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Какие модели и почему выбраны для решения\n",
    "\n",
    "### Embedding: `all-MiniLM-L6-v2`\n",
    "\n",
    "Даёт приемлемое качество при минимальных ресурсах и не требует доменной адаптации.\n",
    "\n",
    "### Re-ranker: `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "\n",
    "Cross-encoder для финального ранжирования: он принимает пару `(query, document)` целиком через full attention и выдаёт единый скор релевантности. В отличие от bi-encoder, который кодирует запрос и документ независимо, cross-encoder моделирует взаимодействие между ними, что даёт +20–40% precision@k. Обучен на MS MARCO (530К query-passage пар). В pipeline вызывается после hybrid search — переранжирует кандидатов с `max_length=512` токенов. Компактный размер (6 слоёв) позволяет использовать на CPU.\n",
    "\n",
    "### Sparse retrieval: BM25 (Okapi)\n",
    "\n",
    "Реализация `BM25Okapi` из `rank-bm25`. Подключается как «страховка» от семантических ошибок dense-поиска: ищет по точным вхождениям токенов, что критично для различения похожих терминов (BACE1/BACE2, GSK3α/GSK3β), которые нейросеть может спутать в эмбеддинг-пространстве. Токенизация — regex-паттерн `[a-zA-Zα-ω][a-zA-Zα-ω\\-]{2,}` (слова ≥3 символов, включая греческие буквы для биохимических терминов). Индекс строится по всем child-чанкам (тот же корпус, что и ChromaDB).\n",
    "\n",
    "### Fusion: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Объединение результатов dense (ChromaDB) и sparse (BM25) поиска. Формула: `score(d) = Σ 1/(k + rank(d))` с `k=60` (стандартное значение из оригинальной статьи). RRF работает только с рангами, не требует нормализации скоров из разных систем. После RRF результаты агрегируются из child-чанков в parent-секции (`children_to_parents`) — берётся максимальный score среди children каждого parent.\n",
    "\n",
    "### Векторная БД: ChromaDB\n",
    "\n",
    "Персистентное хранилище (`PersistentClient`) с HNSW-индексом (cosine space). Выбор обусловлен: встроенная поддержка Python без сервера, метаданные хранятся вместе с эмбеддингами, батчевая вставка (по 500 документов). Архитектура **parent-child**: индексируются только child-чанки (300 символов, overlap 50), а при retrieval возвращаются полные parent-секции — это даёт точность мелких чанков при полноте больших контекстов.\n",
    "\n",
    "### Чанкинг: `RecursiveCharacterTextSplitter`\n",
    "\n",
    "Из `langchain-text-splitters`. Параметры: `chunk_size=300`, `chunk_overlap=50`, разделители `['. ', '; ', ', ', ' ', '']`. Рекурсивная стратегия сначала пытается разделить по предложениям (`. `), затем по всё более мелким единицам. Минимальная длина чанка — 40 символов (`MIN_CHUNK_LENGTH`). Размер 300 символов оптимален для `all-MiniLM-L6-v2` (max sequence 256 токенов ≈ 300–400 символов).\n",
    "\n",
    "### LLM: `Qwen2-1.5B-Instruct` (по умолчанию)\n",
    "\n",
    "По умолчанию ставим Qwen2-1.5B-Instruct: он достаточно умён, чтобы следовать структурированным инструкциям (XML-токены `<CONTEXT>`, `<SOURCE>`, `<QUESTION>`, `<INSTRUCTION>`) и цитировать источники в формате `[PMID:X]`. Поддерживает chat template (`apply_chat_template`), что упрощает формирование промпта. В каталоге также доступны:\n",
    "\n",
    "| Модель | Размер | CPU RAM | Качество | Примечания |\n",
    "|--------|--------|---------|----------|------------|\n",
    "| TinyLlama 1.1B | 1.1B | ~5 GB | 2/5 | Самая лёгкая, низкое цитирование |\n",
    "| **Qwen2 1.5B** | 1.5B | ~7 GB | 3/5 | **По умолчанию**, лучший для CPU |\n",
    "| Phi-2 2.7B | 2.7B | ~11 GB | 3/5 | Нет chat template (plain prompt) |\n",
    "| Gemma 2 2B | 2.6B | ~11 GB | 3/5 | Gated, нужен HF token |\n",
    "| Mistral 7B | 7.2B | ~30 GB | 4/5 | Лучший баланс (GPU 8+ GB) |\n",
    "| Qwen2 7B | 7.6B | ~32 GB | 4/5 | Отличное следование инструкциям |\n",
    "| Llama 3.1 8B | 8.0B | ~34 GB | 5/5 | Лучшая faithfulness (gated) |\n",
    "\n",
    "Поддерживается квантизация: FP16, 4-bit NF4 (bitsandbytes), 8-bit — для запуска крупных моделей на GPU с ограниченной VRAM.\n",
    "\n",
    "### Почему не более крупная модель по умолчанию\n",
    "\n",
    "Модели 7B+ дали бы лучшее качество генерации и цитирования, но требуют мощную видеокарту (8+ GB VRAM) или 30+ GB оперативки.\n",
    "\n",
    "\n",
    "### Подсчёт токенов: токенизатор модели с tiktoken fallback\n",
    "\n",
    "Для контроля бюджета токенов при сборке промпта используется `count_tokens()`: сначала пытается подсчитать через токенизатор загруженной LLM (`tokenizer.encode`) — это даёт точный результат для конкретной модели. Если модель ещё не загружена (например, на этапе сборки промпта до инициализации LLM), используется `tiktoken` с кодировкой `cl100k_base` как fallback.\n",
    "\n",
    "### Structured Prompt: XML-токены\n",
    "\n",
    "Для чёткого разделения контекста, вопроса и инструкций используются XML-подобные токены-разделители (`<CONTEXT>`, `<SOURCE pmid=\"...\" section=\"...\" year=\"...\">`, `<QUESTION>`, `<INSTRUCTION>`). Модели (особенно Qwen, Llama, Mistral) обучены на данных с XML/HTML-разметкой и хорошо распознают структурные теги.\n",
    "\n",
    "### Метрики качества: proxy-оценки без human evaluation\n",
    "\n",
    "Пять автоматических метрик для оценки качества ответов:\n",
    "\n",
    "| Метрика | Формула | Что измеряет |\n",
    "|---------|---------|-------------|\n",
    "| Faithfulness | cited_sentences / total_sentences | Доля предложений с цитатой `[PMID:X]` |\n",
    "| Source Coverage | cited_pmids ∩ source_pmids / source_pmids | Какую долю найденных источников модель процитировала |\n",
    "| Citation Accuracy | cited_pmids ∩ source_pmids / cited_pmids | Все ли цитаты ссылаются на реальные источники |\n",
    "| Context Similarity | mean(chunk_similarity) | Средний retrieval score чанков |\n",
    "| Answer Relevance | cos_sim(embed(query), embed(answer)) | Семантическая близость вопроса и ответа |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
